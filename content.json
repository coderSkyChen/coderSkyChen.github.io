[{"title":"AlphaGo初探","date":"2019-04-01T08:22:43.000Z","path":"2019/04/01/first-look-alphago/","text":"4月1号，作为前瞻实验室强化学习讨论班的轮值小老师，我给同学们分享了主题为“强化学习与AlphaGo”的讲座。小课堂现场： 以下为PDF版本的课件：","tags":[{"name":"强化学习","slug":"强化学习","permalink":"https://coderskychen.cn/tags/强化学习/"}]},{"title":"News,一篇论文被ICME-2019录用！","date":"2019-03-11T06:52:59.000Z","path":"2019/03/11/icme19/","text":"论文“Predictability Analyzing: Deep Reinforcement Learning for Early Action Recognition”被ICME-2019录用，其中我们以强化学习的形式从可预测性的角度做了一些工作。","tags":[{"name":"公告","slug":"公告","permalink":"https://coderskychen.cn/tags/公告/"}]},{"title":"浅谈策略梯度在视觉描述任务中的应用","date":"2019-03-01T02:10:12.000Z","path":"2019/03/01/pgvc/","text":"强化学习中的策略梯度在视觉描述任务的应用。最近在系统性的回顾和整理强化学习，之前做过一段时间的视觉描述任务，而强化学习在该任务中的效果很不错，是从序列决策角度来解决视觉问题的一个典范吧。 强化学习关注求解一个策略，什么样的策略呢？一个可以使在agent与环境的交互过程中取得尽可能大的奖励的策略。那在视觉里边其实也是有很多任务是典型的序列决策模式的，比如： 视觉描述，无论是image caption还是video caption，序列决策主要体现在文本生成的过程中。 视频理解相关任务，跟踪，摘要，关键帧，早期识别等等，视频本身来说就是一种有序列性质的数据结构，因此很多视频相关的任务都可以应用RL建模。 传统方法：交叉熵损失下的文本生成我们假设生成文本的长度为$T$，在交叉熵损失下，我们希望每一个时间步$t$生成的单词尽可能和ground truth一致，这种设置下每个时间步(单词)都会产生一个loss，最后优化目标是总loss最小。$$L(\\theta)=- \\sum_{t=1}^T log(p_{\\theta} (w_{t}))$$其中，${w_{1},…,w_{T}}$是目标生成语句(ground truth)。 强化学习：策略梯度下的文本生成我们首先来回顾一下什么是策略梯度。 策略梯度，顾名思义是一种策略的搜索方法，只不过是依据梯度进行搜索。搜索的目标自然是最大化累积回报。目标函数$J(\\theta)$关于策略参数$\\theta$的导数为：$$\\frac{\\partial J(\\theta)}{\\partial \\theta}= \\frac{\\partial}{\\partial \\theta} \\int p_{\\theta}G(\\tau)$$$$= \\int p_\\theta (\\tau)(\\frac{\\partial}{\\partial \\theta} log p_\\theta (\\tau)) G(\\tau) d \\tau$$$$= \\mathbb{E}_{\\tau \\sim p_\\theta (\\tau)} [\\frac{\\partial}{\\partial \\theta} log p_\\theta (\\tau) G(\\tau)]$$其中$\\tau$指的是某一条完整轨迹，$G(\\tau)$指的是该条轨迹的累积回报。上述公式主要是做了一个log变换，log有一个很好的属性就是可以把连乘变成加法，而且这样变换之后出现了一个期望，我们喜欢期望，因为有了期望我们就可以通过有限样本的采样去逼近它(大数定理)。 我们可以看到上式的关键是$\\frac{\\partial}{\\partial \\theta} log p_\\theta(\\tau)$,轨迹是由多个step组成的，因此可以方便的展开：$$\\frac{\\partial}{\\partial \\theta} log p_\\theta (\\tau)=\\frac{\\partial}{\\partial \\theta} log(p(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t,a_t))$$$$=\\frac{\\partial}{\\partial \\theta}(log p(s_0) + \\sum_{t=0}^{T-1}log \\pi_\\theta(a_t|s_t)+log p(s_{t+1}|s_t,a_t))$$$$=\\sum_{t=0}^{T-1} \\frac{\\partial}{\\partial \\theta} log \\pi_\\theta (a_t|s_t)$$ 上述第二行的化简把状态转移概率矩阵消除了，这是因为我们没有对环境建模(model-free)，状态转移概率与策略无关。至此我们就完成了策略梯度的求导过程，我们可以发现整个过程都是可导的。 那么怎么应用到视觉生成的任务中呢？主要是以下几点： $\\tau$ 对应一个完整描述(生成的句子) $G(\\tau)$ 对应该句话的得分(和评价指标有关，比如BELU) 小细节：baseline的引入以及探索-利用的均衡。简单的策略梯度是针对整个轨迹来说的，因而回报的方差会很大，为了降低方差，往往会引入一个baseline，这里不做具体展开。探索-利用的平衡是强化学习重要的主题，一种常见的方法是依据action的概率去采样动作，而不是直接argmax来得到动作，这个方法非常work。整体网络结构不会有变化，每个时间步依然是通过softmax层得到对所有单词的概率估计，但是loss的计算很不一样了，策略梯度下的loss是整个轨迹的得分，而交叉熵是句子中每个单词得分损失之和。 两种方法的对比交叉熵损失 本质是单词级的优化 不能保证损失函数和评价目标严格相关，比如交叉熵损失小，但是BELU得分不一定就高。 训练和测试不一致问题，训练时输入的是上一时刻的ground truth，来作为上一步的上下文信息。但是测试的时候没有ground truth，只能使用上一步预测的单词。 策略梯度 句子级别的优化 直接优化评价目标($G(\\tau)$) 训练测试一致","tags":[{"name":"强化学习","slug":"强化学习","permalink":"https://coderskychen.cn/tags/强化学习/"}]},{"title":"所谓误解","date":"2019-02-27T09:33:28.000Z","path":"2019/02/27/wj/","text":"所谓误解。 虽然我年纪也不大但是我也是越来越觉得人们的争执很大的部分都来自于误解而这误解的本质，我猜测是以自己的模式去度量别人毫无疑问在最开始的时候我们都是怀着最大的善意","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"知识蒸馏三部曲：从模型蒸馏、数据蒸馏到任务蒸馏","date":"2019-02-23T03:44:24.000Z","path":"2019/02/23/distilling/","text":"最近看到了任务蒸馏的论文，顺便整理一下蒸馏方向的文章。我了解到“蒸馏”一词最早出现在15年Hinton的论文中。 模型蒸馏：Distilling the Knowledge in a Neural Network我们知道模型集成是效果提升的一个很直观的方法，比如参加一些比赛的时候用来最后的提升，但是普通的集成方法往往需要多个模型，消耗的计算资源会比较多。这篇文章提出了“distillation”方法来缓解这个问题。 集成模型通常有较好的泛化能力，而蒸馏就是把复杂的集成模型的能力迁移到小模型中的一种方法。 方法：训练小网络拟合复杂模型的输出（也可以加入原任务的loss进行多任务训练）。$q_i = \\frac{exp(z_i/T)}{\\sum_j exp(z_j / T)}$，这个是softmax函数，完成从网络输出的logtis到probabilities的映射。这里的T代表温度，通常默认1，对于分类任务来说使用T=1往往会导致不同类的概率差距很大，过度集中于某一个类，其他类别的信息难以利用。为此该文章特别强调增大温度的设置值，以此来增强其他类别的信息。这个比较好理解，增大温度相当于e的指数项变小了，因此不同类别的差异也变小了。 蒸馏法可以这样形象的理解，增大温度T，将无关重要的数据结构蒸发，只留下我们在意的抽象关联（特征）。 复杂模型的输出透露的信息很重要，比如哈士奇、拉布拉斯和橘猫这三种类别，自行脑补吧。 数据蒸馏：Data Distillation: Towards Omni-Supervised Learning这个是FAIR的文章，开篇就来一句：“omni-supervised”，这个和“开局一把刀，开局一只鸡”有什么区别。。。 我们先不管什么omni-supervised这种挖概念的，这篇文章整个是围绕数据蒸馏来的。什么是数据蒸馏呢？简单来说，模型不集成了，只要一个模型就可以，我变换数据然后就能得到相应的结果，把这些结果集成起来扔给学生模型去学习。 数据蒸馏有四个步骤： 在有标签的数据集上训练一个模型 将无监督的数据通过多种数据变化后输入模型 结果集成，得到伪标签 使用有监督数据和有伪标签的无监督数据重新训练模型 任务蒸馏：DistInit: Learning Video Representations without a Single Labeled Video这个是CMU和FAIR合作的论文。我感觉比之前的数据蒸馏硬核许多。 训练一个好的视频理解模型往往需要大量的视频标注数据集，然而不同于图片，视频标注本身就困难需要因为有许多帧都要过一遍才行。我们知道ImageNet预训练的网络对图像有很好的层级化表征，但是目前比较先进的方法I3D，要求视频理解模型与ImageNet预训练模型有相似的结构，而这个约束是很严格的，比如R(2+1)D就不能适用。本文的思想是使用ImageNet预训练的模型作为teacher，训练视频理解模型。 方法：首先有两类数据集：A和B，A是无监督数据集，B是有监督数据集。在A中训练视频理解模型，损失函数是交叉熵：ImageNet预训练模型输出的概率与student模型输出概率。然后在B的训练集中finetune student模型，在B的测试集上测试模型。 一个关键的问题是在A训练student时，teacher的输入是一张图片，如何确定选择输入短视频中的哪一帧作为teacher的输入呢？文章测试了：随机采样、中心采样和多个采样点求均值的方法。 最后的一个结论是，这种teacher-student的蒸馏框架比inflated(I3D)的方法好很多，但是比起在更大的监督数据集上的预训练依然有差距，但是这个是可以接受的，至少本文提出了一种无监督的更好的视频理解模型的初始化方法，是有贡献的。","tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://coderskychen.cn/tags/深度学习/"}]},{"title":"《西西弗神话》与荒诞","date":"2019-02-17T04:31:46.000Z","path":"2019/02/17/xxf/","text":"再看《西西弗神话》，有了新的体会。最早可能是在高中的时候读过《西西弗神话》，当时就是单纯的觉得好无聊啊，看不懂。最近再次读到时，却有了新的感受，我们和西西弗没有本质的区别，西西弗一次又一次的把石头推上山顶，而我们呢？其实我们每个人都有自己的石头，然后我们想尽办法把这个石头推上去，推完一个石头之后，马上又来了一个新的石头，我们乐此不疲的如此循环往复。我觉得推石头本质上是人类在寻求内心的平静，我们都希望把自己放在一个可以安心的位置，于是我们做了很多功。谈到内心的平静时，我常常会想起西方祭奠时常用的rest in peace，在有生之年就得到内心平静的人是幸福的。接下来说荒诞，首先我觉得大部分人都活在自我欺骗之中，否则我们根本就寸步难行。（不是出于悲观主义，而是非常客观的）比如有几个基本的问题我们就没有答案： 我是谁？ 其他人有意识吗？我的意识是自由的吗？我有意识吗？人的荒诞感在于人们不余遗力的生活，却不知道生活或者工作的真正意义是什么。“当人意识到人生没有目的的时候，对目的的本能渴望和没有目的的现实就会发生强烈的冲突，进而让人产生荒诞感。”形而上学希望找到终极真理，然后用以指导生活，于是在形而上学下，人生的意义就是寻找终极真理。但是，如果，人生，世界本身就是没有目的的呢？如果形而上学是不存在的呢？如萨特所说，“世间万物的存在都是偶然的”，在这种假设下，人来到世界上本身就没有什么使命，没有什么特别的，就这么存在了。这种时候荒诞感油然而生，我们才会发现其实金钱荣誉地位皆为梦幻泡影，那么我们的奋斗目标怎么办？岂不是盲目的？从众的？世俗的？那进一步就会得到，我们为什么活着？《哲学家们都干了些什么》中提到，怎么解决荒诞感呢？怎么样才能显得我做的一切不是没有意义的呢？主要的途径是：自己为自己建立一个体系，这个体系有自己的核心价值观，指导我们的生活工作，这个体系有它自圆其说的“意义”，比如：信仰宗教，信仰善良，帮助弱小，让自己的存在感更强等等。比如如果在大街上随便问一个老大妈，你觉得生活的意义是什么？答案可能是“让子女生活的更好”，这就是她自己的体系，她自己的哲学。","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"再见了，2018","date":"2019-02-17T03:54:09.000Z","path":"2019/02/17/goodbye2018/","text":"2018年度总结。一直想写，今日心情闲适舒畅，于是提笔。2018年是我的本命年，但是现在回想起来，其实过得还算顺利的。2018年比较重要的一件事应该就是找工作了，我在重要的事情上从来不含糊，大三保研的时候就悉心准备，以致于后来我妈妈说，“我当时还担心你打游戏不上心正事”，我说，怎么可能呢，你还是不了解我哈哈。同样，找工作这件事我早早地就开始准备了吧，从3月份开始，复习算法，ML和DL，准备简历，参加找工作经验交流会，最后先后拿到了京东、美团、百度、微软和腾讯的offer，其中要特别感谢在找工作过程中帮我内推的师兄们，WJ、HZJ和LH，感谢！对杰哥有点惭愧，最后没去MS有点不好意思，但是人家肯定也不缺人才。感受最深的一点是：机会需要自己主动抓取，当时在群里看到一个姐姐发布实习生的招聘帖子，由于我觉得工作描述和我现在做的东西很接近，是我感兴趣的，而且我个人是长期看好相关方向，于是我主动联系请缨校招面试，这才有了后续。感情方面，2018年没有什么进步（也不能说退步），主要是我反而更享受一个人了。。就像Sheldon说的“我觉得自己足够有趣了”，况且还有好多paper等着我看呢，还有好多事情在排队，我也乐在其中。","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"温故知新：重读R-CNN，Fast R-CNN和Faster R-CNN","date":"2019-02-15T10:23:56.000Z","path":"2019/02/15/wgzx-0/","text":"R-CNN，Fast R-CNN和Faster R-CNN是目标检测任务中非常经典的三部曲，每一个工作都能给人留下非常深刻的印象，近似于一种艺术，近期再次拜读，有许多收获。 概述三种算法R-CNNR-CNN名字由来：“Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features.” 所以我们知道R is for region.算法流程： 使用selective search 得到候选区域 把候选区域缩放到$227^2$ 使用CNN获取固定长度的特征 把特征送入SVM分类，送入mlp回归修正bboxFast R-CNN，强调了特征共享以及分类回归多任务训练算法流程： 使用SS得到候选区域 整图送入CNN，得到feature map 在feature map上做RoI Pooling 分类与回归整合在一起Faster R-CNN，彻底的端对端训练，使用RPN学习proposals算法流程： 整图送入CNN，得到feature map 设计RPN（Region Proposal Network）获取候选区域：思想是使用一个小网络在特征图上滑动，在特征图每一个位置都预测K个锚点的前景概率和位置，通过前景概率和NMS完成第一个阶段的bbox的回归修正，并得到topN的proposals。 在特征图中做RoI Pooling 分类+回归 收获关于Bbox回归，为什么可以回归？如何回归？Bbox回归可以看做是从输入到输出的一种映射，其中输入为：proposal区域的特征和proposal的坐标，输出为：修正后的proposal坐标。文中使用线性函数完成坐标的映射，那么为什么我们可以学习到这种映射呢？关键在于我们拥有proposal的特征，即：the feature knows，比如对于一只动物，它在$200^2$中的位置不同，那么我们得到的特征也会体现出不同，这就是我们可以学习的根源。那么如何学习呢？思想就是：通过平移和缩放变换，因而学习的目标就是平移量和缩放量。 特征共享特征共享最早应该是SPPNet提出来的，大部分的proposals有很多的交叉，因此单独对proposal计算前馈特征会有许多无用功。 Fast R-CNN也是一种多任务学习算法使用NN的两个分支完成分类和回归两个任务，是一种很创新的思路。 RPN让人惊艳之前的两个算法都没有摈弃SS获取proposals的路子。到了Faster R-CNN中，作者提出了RPN，使得连proposals都可以从网络中获得，RPN的输入是特征图，输出是proposals。 RPN思想：使用一个小网络在特征图上滑动，预测在某个位置上时，bbox区域属于前景概率同时回归一波。 RPN实现：使用3*3卷积增大中心像素的感受野，随后使用1*1卷积实现全连接网络的滑动，非常巧妙，比如：特征图：C:512,h:37.w:50，1*1卷积通过输出通道的设置，巧妙的实现了全连接网络在特征图每个位置滑动的过程。综上，RPN的思想和实现都是让人感到精巧。什么是锚点（anchor）？如何起作用？文中称“We introduce novel “anchor” boxes that serve as references at multiple scales and aspect ratios.”锚点是RPN应对多尺度的一种策略，锚点具体来说就是一组bbox坐标。在文中作者枚举了9种锚点。锚点的思想是：在特征图的每一个位置上预测K个bbox，而这K个bbox又有不同的形状先验，在bbox回归时进行修正；这样做的合理性在于：特征图每个点都在原图中有一定的感受野。Faster R-CNN的最小分辨率由于Faster R-CNN是基于特征图获取proposal的，如果基网络为VGG，那么最小分辨率为16*16，因为有4个pool，对于特征图某个位置来说，原图中的对应16个像素都会映射到这个位置上是无法区分开的。锚点与感受野我注意到论文中提到VGG的在conv5_3的感受野是228，即使是33卷机之后，感受野228+162也是很小的，而锚点的尺寸中，最大的有512*512的，我觉得这是很不合理的，因为明显的大于了感受野区域，所以我觉得faster-R-CNN对于大物体来说应该是很不准的。","tags":[{"name":"温故知新","slug":"温故知新","permalink":"https://coderskychen.cn/tags/温故知新/"}]},{"title":"《夜空中最亮的星，是一个有毒的BGM》","date":"2019-02-02T03:23:22.000Z","path":"2019/02/02/first-lover/","text":"看来我无聊的时候很无聊。我无聊的时候就喜欢删朋友圈今天又打算来个彻底的清理打开我的朋友圈发现最早的一条状态是NULL太好了这样我就可以删到这一条之前啦这样就省了一些麻烦比如：全部删完之后还得担心别人会不会怀疑被我屏蔽了这个随笔存在的意义是记录初恋分手的状态201710122317我是很有仪式感的况且初恋失败是我的一大遗憾遗憾的不是具体的失恋对象而是没能从初恋走到最后这件事两个人，一路颠簸，同步成长是我最向往的状态只是，我再也没有这个机会了","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"《后来，凉薄成了我的底色》","date":"2019-02-02T03:03:59.000Z","path":"2019/02/02/wt/","text":"你的底色又是什么样的呢？在最近的地方时间抚摸我灵魂的动作也轻柔了许多没有耀眼的阳光暗影就顺势拥抱了我 —纪念2017-09-19-16：36","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"看《哲学家们都干了些什么》有感","date":"2019-01-28T09:53:41.000Z","path":"2019/01/28/zxgk/","text":"好书安利~ 首先我觉得，哲学是人类试图自我和解的一种挣扎。世界上有很多科学解答不了的问题，而哲学就是对这些问题的讨论。有的人对这些问题没有疑惑，要么是因为有了自洽的解释，要么是有了真诚的信仰。然而，偏偏有一群人，无法自洽，他们需要答案，需要剔除飘飘无所依之感，他们需要自我和解。","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"《你离开了南京，从此没有人和我说话》","date":"2019-01-27T05:43:45.000Z","path":"2019/01/27/dldld/","text":"听一首歌，居然能失眠。 昨晚看了看电影，刷了刷抖音，时间已到两点，睡觉之前很不幸，听到了《你离开了南京，从此没有人和我说话》，简单的旋律，忧伤无以复加，只感到落寞落寞落寞，一股悲情在胸口郁结，偏偏又没有眼泪，随手翻了翻评论，彻底失眠。","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"《我不是药神》","date":"2018-07-08T05:19:23.000Z","path":"2018/07/08/yaoshen/","text":"观后感。 闪烁的目光是“我”此生的满天繁星","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"News,一篇论文被BMVC-2018录用！","date":"2018-07-03T00:07:00.000Z","path":"2018/07/03/bmvc18/","text":"论文“Not all words are equal: video-specificinformation loss for video captioning”被BMVC-2018录用，其中我们引入“information loss”从损失函数的角度缓解caption数据集中单词分布倾斜导致的万能句子问题。","tags":[{"name":"公告","slug":"公告","permalink":"https://coderskychen.cn/tags/公告/"}]},{"title":"《靠岸》","date":"2018-06-01T09:06:56.000Z","path":"2018/06/01/pull-over/","text":"我觉得靠岸和rest in peace其实是一样的，这不就是我们一生所求吗？ 风尘仆仆的人都想找到自己的摆渡人然后靠岸，卸下长久以来的不安可是今天我觉得每个人都是自己的摆渡人有愧疚的，请献上你真诚的歉意温热的泪水会洗涤你的斑驳有痛苦的，钥匙一直都在自己手上这，你知道的","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"书单2017","date":"2018-05-28T14:14:32.000Z","path":"2018/05/28/booklist-2017/","text":"列出了一下2017年看过的不错的书籍。 《三体》《时间移民》《白夜行》《摆渡人》《我与地坛》《看见》《秘密》《嫌疑人X的献身》《好吗好的》《玩命爱一个姑娘》《梦幻花》《第三次拯救未来世界》《解忧杂货店》《岛上书店》《湖畔》《恶意》《我的孤独是一座花园》《变身》《随遇而安》《半小时漫画中国史》《我决定活的有趣》《80天环游世界》《红：陪安东尼度过漫长岁月》《偷心，为什么爱情总让人如此疯狂》《时间的形状：相对论史话》《流浪地球》《星空的琴弦：天文学史话》《一个陌生女人来信》《羊脂球》《重生》《群山之巅》《人间失格》《假性亲密关系》《柔软的宇宙》《平行宇宙》《1984》《胡兰成自传：今生今世》《宇宙超度指南》《穿越时间可能吗？》《活着》","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"《意义》","date":"2018-05-22T14:52:01.000Z","path":"2018/05/22/walking/","text":"某一时刻，我突然感觉到了这就是意义。 修行，大概就是一双热泪盈眶的眼睛感动着另一双眼睛","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"简析《Iterative Visual Reasoning》中的局部推理","date":"2018-05-10T08:58:23.000Z","path":"2018/05/10/local-module/","text":"上周组会准备了一个PPT，力求形象的解释局部推理过程中的记忆更新流程。PPT传送门：a closer look at iter-reason","tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://coderskychen.cn/tags/深度学习/"},{"name":"视觉推理","slug":"视觉推理","permalink":"https://coderskychen.cn/tags/视觉推理/"}]},{"title":"《pytorch复现：Iterative Visual Reasoning Beyond Convolutions》","date":"2018-05-02T14:55:12.000Z","path":"2018/05/02/iter-reason/","text":"复现与启迪。FeiFei和陈鑫磊合作的论文：《Iterative Visual Reasoning Beyond Convolutions》被评为 CVPR2018 spotlight，这篇文章是深度学习与推理方面的开山之作，主要包括一个spatial reasoning以及global reasoning。 spatial reasoning：使用显式的memory模块，这个和FeiFei组最近论文中倡导的一致，即：借助显式记忆完成推理。 global reasoning：考虑了3种关系：类间关系、区域实体与类的关系以及区域实体之间的关系。 带给我们的启发： 学会选择合适的数据集、自定义恰当的任务来最大程度的体现我们要打的靶子。 写论文中顶会，一定要针对前沿痛点，可能比较难甚至没有做过，但是哪怕我们能在这个方向上前进一点点，能给别人以启迪，这绝对是有价值的工作。 厚积薄发，详略得当。这篇文章从代码角度来讲其实包含很多模块，如果在文章中一板一眼的都写出来，那么我觉得仅仅是spatial部分就能写好几页，而作者处理的非常好，虽然有些细节单单从文章来看不清晰，但是整个pipeline我们是能够想象出来的。而且其中好多细节都离不开积累，比如：spatial memory如何更新？作者借鉴了GRU（一种循环神经网络）的思想，再比如memory的形状类似于图片，所以在更新GRU各个门的时候使用的是卷积操作，这些小细节可能是方法能work的重要因素。 对迭代的思考： 我认为这篇文章中的迭代更多的是通过boosting的形式优化模型，从偏差-方差角度分析，减小了偏差，所以迭代会带来提升。17年的spatial memory文章的迭代其实更有“道理”，因为它是序列化的预测目标，而本文是一次迭代预测所有区域。 下一步考虑扩展到视频推理中。最后，附上我的复现代码：Iterative-Visual-Reasoning.pytorch","tags":[{"name":"reimplementation","slug":"reimplementation","permalink":"https://coderskychen.cn/tags/reimplementation/"},{"name":"深度学习","slug":"深度学习","permalink":"https://coderskychen.cn/tags/深度学习/"},{"name":"pytorch","slug":"pytorch","permalink":"https://coderskychen.cn/tags/pytorch/"},{"name":"region recognition","slug":"region-recognition","permalink":"https://coderskychen.cn/tags/region-recognition/"},{"name":"spatial reasoning","slug":"spatial-reasoning","permalink":"https://coderskychen.cn/tags/spatial-reasoning/"}]},{"title":"《常见的动作识别算法实现》","date":"2018-03-15T14:38:12.000Z","path":"2018/03/15/action-zoo/","text":"一点总结。以Something-something为例，在pytorch平台上实现了：Two-stream，TSN，C3D，I3D。 主要的目的不是刷分，而是走一遍各个模型，熟悉它们的pipeline以及代码的积累。 代码传送门，嗖~ 比较伤感的是我们提出的DIN的方法还不能放出去，因为要申请专利，哎。。（P.S. 虽然DIN 被IJCAI拒了，但是这个工作本身确实有点意思的）","tags":[{"name":"动作识别","slug":"动作识别","permalink":"https://coderskychen.cn/tags/动作识别/"},{"name":"reimplementation","slug":"reimplementation","permalink":"https://coderskychen.cn/tags/reimplementation/"},{"name":"深度学习","slug":"深度学习","permalink":"https://coderskychen.cn/tags/深度学习/"},{"name":"pytorch","slug":"pytorch","permalink":"https://coderskychen.cn/tags/pytorch/"}]},{"title":"《和解》","date":"2018-02-14T12:58:43.000Z","path":"2018/02/14/jidong/","text":"或许人类的究极精神追求就是自我和解？总有一些剧情轮到我们去单刀赴会紧张的，激动的，快乐的，未知的那时候我不得不带着这一路以来的修炼一身的勇气以及你们的鼓励去迎接它无论是更加张扬还是永远沉寂那时候文心湖底的每一窝湍流和天穹里的每一个音符都会与我和解","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"《像他一样的人》","date":"2018-01-10T12:58:06.000Z","path":"2018/01/10/poetry-likehim/","text":"也许希望成为这样的人？他的眼底有岁月涓涓流过的痕迹他的眼角有阅尽世间百态的微笑他的眉峰氤氲着关于青春的故事但我最爱的是他身后的那片海洋","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"《庄严的时刻》","date":"2018-01-02T12:18:46.000Z","path":"2018/01/02/poetry-solemn-moment/","text":"你也有过这样的时刻吗？鲜黄色的裙子让人一眼就能看出来，而这双高跟鞋发出的滴答声，仿佛和她的心跳一个节奏；她抱怨着时间太快了，总觉得脸上的妆还没涂匀；唯一让她感到满意的，是她那散着香气的长发；是的，他说过，“你身后的风有一种芳香”","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"理解胶囊网络：《Dynamic Routing Between Capsules》","date":"2017-12-09T09:33:40.000Z","path":"2017/12/09/capnet/","text":"摘要：结合知乎高票答案分析总结了Hinton最近提出的胶囊网络，斗胆感觉Hinton更多的是给了Capsule一个漂亮的说辞，似乎除了动态路由以及神经元分组之外与目前的网络结构没有本质区别。 Capsules是什么？capsule是capnet（胶囊网络）中的最小单元，类比于neuron（神经元）在神经网络中的“水平”。 论文摘要的理解 A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. 这是论文中的第一句话，一个胶囊是一组神经元，其激活后的向量表征了一种特定的实体，这个实体可以是一个object，也可以是object的一部分 从这里可以分析：一个神经元的输入是一个个的标量，那么一个胶囊的输入则是一个个向量。 使用激活向量表征实体的属性，所谓属性，例如：姿态、形变、速度、反射率、色相、纹理等等，其中“是否存在”是一种特殊的属性 向量模长表示实体存在的概率 向量方向表示其他的属性 某一层的胶囊通过矩阵变换做出预测，预测结果作为高层胶囊的输入 胶囊层之间的连接权重通过可迭代的一致性路由机制更新 简单理解路由机制：低层胶囊i与高层胶囊j的输出的一致性越大那么它们之间的连接权重也要迭代地增大。 Capsule与neuron的联系与区别？ 该图引自：廖华东PPT，他的github capsule是强化版的neuron， 从标量升级到向量，而且capsule是capnet中的最小单元。 capsule的输入经过仿射变换：$W_{ij}u_j$之后，称为预测输出 （知识点啊！） 胶囊层之间的连接权重通过动态路由算法更新。（此部分不需要BP,属于无监督） 非线性激活函数称为sqush，也是一个向量级别的，输入向量方向不变，同时把模长约束至0~1如何计算一个capsule的输入输出？以capsule $v_{j}$为例，定义其输入为：$s_j$,那么从输入到输出的公式为：$$v_j=\\frac{||s_j||^2}{1+||s_j||^2} \\frac{s_j}{||s_j||}$$ 该公式的第一项对模长进行约束，第二项其实就是输入向量的单位向量作用：保留方向 这个公式同时是capsule的非线性激活函数$s_j$是所有低层capsule预测向量$\\hat{u}_{j|i}$的加权和：$$s_j=\\sum_i c_{ij} \\hat{u}_{j|i} $$$$\\hat{u}_{j|i}=W_{ij}u_i$$低层胶囊$u_i$首先乘以$W_{ij}$进行仿射得到其预测向量$\\hat{u}_{j|i}$,然后以加权和的形式联合所有低层胶囊的预测向量得到高层胶囊的输入$s_j$。权重$c_{ij}$的计算：对 $b_{ij}$在第二维上做一个softmax，：$$c_{ij}=\\frac{exp(b_{ij})}{\\sum_k exp(b_{ik})}$$ 对于低层胶囊i来说，它要向所有的高层胶囊发送信息，所以是在第二维度上进行softmax 其中的b即是通过动态路由得到的什么是动态路由算法？它在网络中处于什么位置？动态路由处于胶囊层之间，对应于结构图中的PrimaryCaps与DigitCaps之间，算法如下图： 该算法描述的是第l层与第l+1层胶囊之间的连接权重计算方法；r为迭代次数 首先将所有的权重初始化为0，然后开始迭代，在每一次迭代中依次完成以下计算： 计算连接权重：$c_{i}=softmax(b_i)$ 计算l+1层的输出$v_j$: $v_j=squash(s_j)$, $s_j=sum_i c_{ij} \\hat{u}_{j|i}$ 更新b: $b_{ij}=b_{ij}+\\hat{u}_{j|i} · v_j$ 值得注意的是，权重的理解应该从低层向高层来看，低层的节点i向所有高层节点发送信息，所以是$\\sum_k c_{ik} =1$, 而不是$\\sum_k c_{kj} =1$ 如何理解《Dynamic Routing Between Capsules》中的网络结构？ 模型结构 模型共有三层：普通的卷积层+胶囊卷积层+胶囊全连接层 普通卷积层+ReLU：卷积核9*9+步长2+输入通道1+输出通道256 胶囊卷积层+胶囊激活函数：卷积核99+步长2+输入通道256+输出通道832，上图中把第二个卷积层的结果分为了32组，每一组厚度8，这样做是为了方便理解下一层的输入是8D的。 胶囊全连接层：上一层得到[6*6*32*8]的向量，有6*6*32=1152个8D节点，而下一层的维度是：10*16，有10个16D的节点，因此其全连接可以认为是在1152与10这两组节点之间产生的。注意图中$W_{ij}$是仿射变换的权重而不是全连接的权重，其中i属于(1,6*6*32)，j属于(1,10),primarycaps的维度：6*6*32*8，通过仿射变换之后，变为了：6*6*32*16，随后进过动态路由算法，得到16*10的digitcaps。 最后一层通过计算digitcaps的向量模长 来表示概率 动态路由机制发生在胶囊全连接层 Capsules的设计理念是什么？Capsules由来 （参考来源） 神经解剖学中的“皮层微柱” 同变性而非不变性。CNN的pooling饱受诟病，比如信息丢失，它带来了一定程度的不变性（Invariance），但是事实上我们更需要同变性（Equivariance），hinton认为CNN前边做的很好，因为是同变性的，例如平移，保留了位置信息 计算机图形学，引入视角不变性，这一点对应着capsule的仿射变换 总结 看了网上几个人复现的代码，隐约感觉Hinton更多的是给了一个更漂亮的解释。。 前两个卷积层和我们目前的CNN是完全一样的（除去激活函数），第二层卷积的输出：primarycaps 8个一组，仅仅就该层而言只是说法上的不同 最大的创新在于动态路由机制与使用向量的模长来表征实体存在的概率 动态路由算法得到的连接权重即为胶囊全连接的权重 保留了原始的CNN结构是为了能够更好地抓住一些底层的信息，比如：边缘 动态路由算法可以看做是一种平行的attention机制，而其权重的更新是无监督的，有点像是聚类的感觉；“平行”是针对低层胶囊的每个节点来说的 整个训练过程包括有监督的BP以及无监督的动态路由算法，感觉hinton在放弃BP上做了努力；动态路由中的参数b是无监督迭代得到的，其他部分都是BP","tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://coderskychen.cn/tags/深度学习/"}]},{"title":"视频描述小总结","date":"2017-12-02T11:33:24.000Z","path":"2017/12/02/videocaption/","text":"本文主要总结了我对videocaption的一点想法或脑洞。。尊重原创，转载请注明出处。 从今年9月份开始，我学了近两个月的视频描述（video caption），后来在11月20号左右决定学一下其他的方向，既然曾经做过video caption，还是写写博客留下点痕迹吧。。 encoder-decoder这样的编解码框架足够好了吗？借鉴于机器翻译，目前video caption主流的方法是CNN+LSTM这样的编解码框架，LSTM充当语言模型，根据上一步的单词，隐状态和当前步的输入来预测当前的输出，这样的方式本身其实并不合理。我们说一句话的时候，脑子里边先是有一个主要的idea，然后围绕着这个idea去扩充句子，最后表达，而不是一个词一个词的想出来的。期待有人能突破编解码这样的框架。。。 attention机制attention机制本质上是一种降维方法，而且一层attention只降低一维。也可以看做是一种加权和，只不过它的权重是学习出来的。视频是一种非常结构化的数据：region组成图片，图片组成视频，因此attention在这里是可以搞很多事情的。但是前一段时间attention有点过热了：spatial attention，temporal attention， 时空attention，多模态attention，结构化attention，多模态特征之间的attention等等。。有一篇论文在attention大军中让我眼前一亮：17年的knowing when to look，第一次点明了attention在caption任务中的问题：在语言模型生成文本的过程中，并不是每一步都需要视觉信息指导的，比如生成 of，the的时候。也就是说，有些单词对视觉信息的依赖很小。 如何让语言模型与视觉模型更好的合作？在做实验的过程中我们发现：以LSTM为基础的语言模型很容易就能学到语言的文法，比如在第一轮的时候几乎所有的预测结果都是：“a man is playing ….”。这是因为 大量的文本中 a后边跟的单词都是man，所以语言模型就学习到了一种统计关联。但是这样的统计关联可能会在一定程度上忽视视觉信息，比如视频中的人是一个女人，如果语言模型不能很好的利用视觉信息，就会预测出man而不是woman。上边提到的Knowing when to look 是较早的针对这个问题提出解决方案的文章，但是我认为这里还有一些做的空间。 如何有效挖掘视频中的时序信息？目前image captio已经做得很不错了，包括大部分image上的任务都在一步步的被攻克，而“视频”是计算机视觉最后的堡垒（引自导师）。从image caption到video caption，我们的数据增加了时间这个维度，而其蕴含的信息也更加纷繁复杂。据我了解提取视频特征的方法大致有两类： 静态特征，视频的每一帧都送入CNN提取特征 动态特征，包括光流，3d卷积，C3D，I3D 提完特征之后通常再接LSTM对时序进一步建模，其实CNN+LSTM就是一个很强的baseline了，CNN能得到非常好的图片表征，LSTM又擅长处理序列数据。通常为了提分，大家会堆特征：CNN+光流+C3D+I3D+音频。我的感觉是，目前在对时序进行建模的时候，我们对LSTM有点过度依赖（比如：多层LSTM，双向LSTM，LSTM之间引入残差连接等等），但是我也没有更好的点子。我的出发点是：有可能计算机并不需要向我们一样去一帧帧的“看”视频。视频表征应该有更有效的方式等待我们去挖掘。举个栗子：cvpr2016一篇论文给我留下很深印象，其关键字：dynamic image，大体是：通过排序学习，将一段视频压缩到一张图片的维度下，而且这个图片看起来非常神奇，我一下子联想到了三体中的”降维打击“ 和”二向箔“。如下图： 强化学习在video caption中扮演重要角色这个任务中主流的loss是交叉熵，结合LSTM的BPTT，在每一个时间步都会有loss反传。但是这个objective函数和最终的评价指标是有一个gap的。强化学习擅长处理这类带有长期决策性质的任务，而caption，其实也是一个长期决策任务：每一步都要决定去选哪一个单词，因此RL在此可以大显身手： 一方面它可以解决传统训练方法的训练-测试过程中的不一致问题（训练时上一步的单词是ground一方面它可以解决传统训练方法的训练-测试过程中的不一致问题（训练时上一步的单词是ground truth，而测试时是上一步预测出来的，会有错误累计问题） 另一方面它直接针对评价指标优化模型参数，针对长期的回报去更新参数 但是，我接触到的强化学习落实到这个任务上的时候，通常只是对loss乘了一个reward（虽然公式推导了很多），据说这只是最简单的强化学习策略，还有提升空间吧。 评价指标有待完善这个现象在使用强化学习之后更加明显，虽然模型预测的结果在诸如BLEU，CIDEr，ROUGE，METEOR指标上有大幅度提升，但是预测出来的话有一个长度变短的趋势，也就是说模型更加保守了，倾向于万能句子。这从侧面说明了这些评价指标并不能很好的反映模型预测结果的好坏，也有可能是强化学习没用好。如何设计更“humanic”的评价指标？ visual-semantic embedding学习很重要文本和图片的embedding的关联学习很重要，目前只发现了一篇做这个的论文，在loss中加入了embedding的相似度损失。我认为这个的重点在于可以进一步利用文本信息，从文本中获得更多的指导。 数据标注不够充足目前主要的数据库是 MSVD和MSRVTT，加一起来共有一万多个视频，数据相对缺乏。因此半监督，无监督，多任务联合学习很重要。","tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://coderskychen.cn/tags/深度学习/"},{"name":"videocaption","slug":"videocaption","permalink":"https://coderskychen.cn/tags/videocaption/"}]},{"title":"为什么要有题目","date":"2017-11-21T11:29:57.000Z","path":"2017/11/21/lowpower/","text":"有意思的话。 过去都是假的，回忆是一条没有归途的路，以往的一切春天都无法复原，即使最狂热最坚贞的爱情，归根结底也不过是一种瞬息即逝的现实，唯有孤独永恒。 生命中真正重要的不是你遭遇了什么，而是你记住了哪些事，又是如何铭记的。 我们趋行在人生这个亘古的旅途，在坎坷中奔跑，在挫折里涅槃，忧愁缠满全身，痛苦飘洒一地。我们累，却无从止歇；我们苦，却无法回避。 所有人都显得很寂寞，用自己的方式想尽办法排遣寂寞，事实上仍是延续自己的寂寞。寂寞是造化对群居者的诅咒，孤独才是寂寞的唯一出口。 生命从来不曾离开过孤独而独立存在。无论是我们出生、我们成长、我们相爱还是我们成功失败，直到最后的最后，孤独犹如影子一样存在于生命一隅。—-《百年孤独》","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"大规模文本分类实践-知乎看山杯总结","date":"2017-08-20T06:53:35.000Z","path":"2017/08/20/zhihucup/","text":"本文主要介绍了我在知乎看山杯机器学习挑战赛中的一些实验和总结，代码已公开，传送门。阅读本篇大约需要10分钟。 尊重原创，转载请注明出处。先晒一发排名，9th，有小遗憾，但是通过这个比赛把深度学习搞文本分类这一套东西走了一遍，还是有收获的。 赛题简述一句话概括：对知乎上的问题进行自动的话题标注，标注个数5个。 参赛者需要根据知乎给出的问题及话题标签的绑定关系的训练数据，训练出对未标注数据自动标注的模型。 标注数据中包含 300 万个问题，每个问题有 1 个或多个标签，共计1999 个标签。每个标签对应知乎上的一个「话题」，话题之间存在父子关系，并通过父子关系组织成一张有向无环图（DAG）。 由于涉及到用户隐私及数据安全等问题，本次比赛不提供问题、话题描述的原始文本，而是使用字符编号及切词后的词语编号来表示文本信息。同时，鉴于词向量技术在自然语言处理领域的广泛应用，比赛还提供字符级别的 embedding 向量和词语级别的 embedding 向量，这些 embedding 向量利用知乎上的海量文本语料，使用 google word2vec 训练得到。 除了对原始文本进行大小写转换、全半角转换及去除一些特殊字符（如 emoji 表情、不可见字符）等处理之外，训练数据和预测数据都没有经过任何清洗。 对于一个问题有四部分的描述：字符级的标题（titlechar），词级的标题（titleword），字符级的描述（dspchar），词级的描述（dspword） 训练集300w的有标注问题，测试集20w的无标注问题，需要我们的模型给这些问题打5个标签 验证集未给出，需要自己划分，我在实验过程中使用了简单的hold-out方法，从300w训练集中留出了10%的数据作为验证集。 1999个label不是互相独立的 数据集下载及其说明，请移步至：。。。 不同网络结构的理解与回顾整体来说，我们主要把这个比赛看成了一个分类问题，即1999个类的分类问题。 我尝试过从问答匹配的角度来做，即：topic看做是question对应的答案，因为topic出了自身的label以外还有一个自身的描述（因此也是一段话）。我认为从问答匹配的角度来看待此问题是更合适的，因为这样可以把topic的描述信息利用上，而分类的角度来做是很难利用这个信息的。但是问答匹配的话面临两个问题： 第一个是正负样例的采样，对于一个问题来说，正样例即为其相应的topic label，而负样例为：剩余其他的label（1999-n），负样例过多需要采取措施。我实验过降采样，即一个question扩展为20个样本，正样本为n那么负样本从1999-n中抽取20-k个 第二个是训练和预测的时间复杂度太高。训练的话相当于原训练集的k倍（k=20），而预测的过程更加耗时，如果不采用粗过滤的方式，那么必须对1999个topic都进行预测得分，才能选出top5，计算量增加了1999倍。很可怕了。所以最终我只是简单跑了一个小demo，没有从问答匹配的角度做本次比赛。 测试过的一些常见元素的组合，比如，TextCNN，VDNN（very deep cnn 17年的论文），LSTM，C-LSTM，双向LSTM，双向LSTM+原始的词向量（三元组），RCNN，attention机制，结构化的attention，多通道trick以及多任务loss（融合LDA主题信息）。 最终结果，我的单模型最高线上0.411分（RCNN+ATTENTION），是我们队的state-of-the-art，与另外两个队友的结果融合之后A榜0.4229，其中模型融合大概能长1个百分点，这里的融合指的是评分矩阵的加权和。 TextCNN该方法是kim在2014年提出的，非常经典的CNN做文本分类的模型。(Convolutional Neural Networks for Sentence Classification ) 卷积核的宽度等于词向量的维度，这样做是有意义的，因为一个完成的词向量代表一个词。而卷积核的高度是人为设定的，一般在1~7之间，这样卷积的含义类似于n-gram的效果，这点不同于图像中的卷积核的尺寸，因为图像是有局部相关性的。 每一个卷积核卷积完后会得到一个m*1的一维feature map，对feature map进行max pooling，这样卷积核的个数就决定了pooling之后的维度，因此顺便也解决了不同文本长度带来的维度不一致问题，当然本身这个问题也可以通过padding来解决。 最后通过一个dropout+fc层使用softmax输出结果完成分类任务。 该论文中，做了一些对比试验，比较了是否使用预先训练的w2v来初始化embedding层，以及是否使用多通道，即：一个通道使用静态w2v，另一个使用可训练的w2v。 VDNN该方法是Yann Le Cun的学生在17年提出来的。(Very Deep Convolutional Networks for Text Classification)主要思想： 虽然一个只有单隐含层的神经网络从理论上有能力学习任何函数，但是实践证明，针对特定问题设计的深层网络能够有效的学习结构化的表征 深层的CNN在计算机视觉取得了好成绩，是因为图像可以由一些底层特征组合出来（点动成线，线动成面），而文本也有类似的结构：字符可以组合成n-grams，stems，words，phrase，sentences 文中提到，TextCNN的一个缺点是需要人工指定卷积核的尺寸，而这个超参数对结果的影响很大，而VDCNN就可以通过深层的结构自动学习到n-gram的组合。 从结构图中可以看到，他们借鉴了残差网络的shortcut机制，但是这个机制是一个可选项，有时候结果并不好。 他们设计网络时遵循的原则： 输出分辨率相同时，feature map的个数也相同 当分辨率减半时，feature map加倍（变厚） 个人感觉没有太新奇的地方。。。难道是人家调参调的好？反正我自己实验的时候结果不如TextCNN，可能是我调参太差劲了。 C-LSTM该方法来源于15年的一篇论文。(A C-LSTM Neural Network for Text Classification) 主要思想：既然CNN 可以获取文本的语义特征，那么何不将这些语义信息经过整理使其依然具有时序结构，然后再送入到LSTM中呢？ 一个重点是reshape feature map，使其保留原始文本的时序，这样送入到LSTM才有意义。 一种filter size对应一个LSTM单元，因为不同的filter size产生不同尺寸的feature map，而feature map的维度决定了LSTM的步数（step） RCNN论文：Recurrent Convolutional Neural Networks for Text Classification 开始我对这个方法的理解有一定的误区，主要是对论文中的循环结构理解有误，应该仔细看一下公式（1）、（2）。它有一个操作，把左右语境的文本移动一位，然后循环的计算左右语义。 我在实现时与原始论文是有改动的，使用了双向LSTM来获取某一个位置的左右语义信息，这样再加上本身的embedding，就组成了一个三元组，即每一个词都可以用这样的一个三元组来表示。 然后再根据论文的公式4，使用一个dense层tanh激活函数，来把这样的三元组映射到一个指定维度，最后采取纵向的maxpooling来获取一句话的表示。使用RCNN的一个感觉是，纵向的maxpooling效果竟然很好。 attention机制我理解的attention机制，就是一个加权和，然后其中的权重是通过学习得到的。keras实现很简单。 记得有一篇HAN的论文，使用结构化的attention来做document的分类，即：单词之间的attention+句子之间的attention。 多通道trick两个通道相当于增加了一倍的计算量和参数量，两个embedding层，一个可以训练，一个固定死。从实验看来不一定会有提升，需要具体情况具体分析。 multi-loss借鉴了17年的video的一篇论文。(CVPR2017)Improving Interpretability of Deep Neural Networks with Semantic Information 首先训练一个LDA模型，获得每个文档的topic分布向量，这样相当于增加了有监督的信息。然后在原有任务的loss基础上，再加一个输出层来学习LDA主题分布。相当于多输出多loss。 得分最高的单模型：RCNN+ATTENTION模型结构图 四个输入：titlechar，titleword，dspchar，dspword，四个embedding层，都可以训练 使用了双向LSTM，最后用LSTM_left:embedding:LSTM_right这样的三元组形式来表示一个词的信息，好处是可以比较好的捕捉到当前词的上下文语义。 使用纵向的maxpooling把k三元组向量规约到一个一维向量上，用以表达一个句子的语义信息。 titlechar和titleword直接连接，dspchar和dspword直接连接，这样连接之后的两组向量分别表示了title和dsp的语义信息。 最后将question represention输入到一个fc层（期望该层能学习到label之间的依赖关系），然后再连接dropout+fc层做一个softmax获取各个类别上的得分分布。 title和dsp语义之间的attention 为什么要title和dsp之间进行attention，而不是四部分都attention呢？我个人的出发点是：word和char能够从不同的级别来表示语义，所以char和word之间使用concat的方式直接拼起来，这样比attention加权和能保留更有多的信息。同时，title还有的信息比dsp更有用（单个实验过，单个dsp得分很惨，也符合常理），所以使用attention机制加权和一下，万一有用呢？233 总结与反思 最后一层激活函数，sigmoid和softmax相比训练速度变慢，sigmoid貌似更适合multi-label，但是我们一直使用的是softmax，在multi-label问题中，我觉得softmax会有一点问题：过于突出向量中的某一个值。 lstm units从256到512会带来提升 通过一个dense层压缩embedding维度，会使结果变差 dsp部分最长的有上万个字符，受限于GPU显存，我的截断设置为：300，大一点可能会好 在我的实验中，LSTM比GRU结果好，而GRU训练速度快，参数相对少。 batchnorm层在有attention的时候放置的位置需要更多的实验，没来及测试BN层 学习率特别重要，我开始训练的模型lr都有一点偏大 一个队友说，我们的一个问题是：模型更迭的速度远大于模型调参的尝试，还是应该多花一些精力调参。。 最后模型融合的时候，融合的权重是我们根据单模型得分人为的指定的，更好的方法是学习出这些权重参数，还是没时间做了… 我一直对引入LDA的multi-loss抱有很大希望，但是在tw单个通道上测试了一下，结果不如单个loss的，然后也就没有继续深究，可能是LDA训练的不好？loss加权设定的不好？ 借鉴残差网络的思想，我应该尝试一下跨层连接，队友貌似有测试","tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://coderskychen.cn/tags/深度学习/"},{"name":"文本分类","slug":"文本分类","permalink":"https://coderskychen.cn/tags/文本分类/"},{"name":"python","slug":"python","permalink":"https://coderskychen.cn/tags/python/"},{"name":"keras","slug":"keras","permalink":"https://coderskychen.cn/tags/keras/"}]},{"title":"时间之外的往事","date":"2017-08-16T12:31:54.000Z","path":"2017/08/16/beyond-time/","text":"有什么东西可以无视时间的洗礼吗？ 我愿与你初春绿蕾慢抽芽我愿与你盛夏白瓷梅子汤我愿与你深秋染霜镀枫叶我愿与你凛冬红泥小火炉 我愿与你执手立黄昏我愿与你剪烛夜已深我愿与你入梦共前尘我愿与你对弈把酒分 我愿与你亲吻微风拂过的山岗我愿与你续写时间之外的往事","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"《我们之间的时间》","date":"2017-08-02T04:06:02.000Z","path":"2017/08/02/poetry-time-between-us/","text":"时间啊，时间。 我清楚的知道从那一刻起你我之间的时间开始一往无前的奔流 但愿你我之间的时间慢下来的时候我们的不羁任性不减当年","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"Learning to Rank with Nonsmooth Cost Functions 挖坑笔记","date":"2017-05-15T13:39:27.000Z","path":"2017/05/15/lambdarank/","text":"摘要：本论文是排序学习领域的又一经典之作，作者站在RankNet的“肩膀”上，向前迈出了一大步：直面排序学习中期望的与实际使用的代价函数不一致问题，提出$\\lambda$函数，使得模型能间接朝着期望的代价函数来更新参数，而且作者还从力学的角度形象的解释了$\\lambda$函数，简直酷炫吊炸天。著名的LambdaRank就是在这篇论文中提出来的。 期望的与实际使用的代价函数不一致问题在机器学习中通常有两类代价函数：期望的代价函数和实际优化时使用的代价函数，前者指能表示我们最终优化目标的函数，而后者指在实际计算时使用的函数，为了方便计算，后者通常要有一些条件约束(比如：平滑的、是凸函数)，这就使得后者往往只是前者的一种近似，一种“妥协”。 本论文站在RankNet的肩膀上，向前迈出了一大步。之前RankNet那篇论文提出了pairwise的概率形式的交叉熵损失函数，其最重要的意义是：只要你这个损失函数对你模型中的参数是可导的，那么就可以愉快的走RankNet的套路了。 But，RankNet的优化目标是：保证每一对的相对顺序正确，而这个优化目标其实只是搜索排序评价指标（比如：NDCG、AP、MAP、MRR、WTA等）的一种“将就”。之所以不直接使用排序评价指标优化，是因为这些排序指标基本上是非凸的、不平滑的，其导数要么是0，要么不可导。 有人会说，我们设计一个和排序评价指标很相似的同时还容易求导的代价函数不就ok了？然而，在排序问题中“排序”这一特性使得这条路很难走通。因此，作者很机智地绕过这个问题，具体怎么绕的？概括来说就是：定义了一个虚梯度，在每次sort之后根据我们的期望再次更新参数。 LambdaRank回顾NDCG参考HappyAngel的博客，NDCG（Normalized discounted cumulative gain）：是用来衡量排序质量的指标，它的目标是：越相关的doc的位置越靠前，同时整体的docs又尽可能都相关。 WTA cost排序评价指标WTA（winner takes all）胜者通吃，具体在信息检索的排序问题中：如果排在首位的doc是相关的，那么 WTA cost=0，否则cost=1。 小例子回顾了这两个小知识之后，下面以一个小例子来说明LambdaRank的思想。 设WTA cost是我们的期望代价，而实际优化使用pairwise error做为代价（也就是预测错了多少对）； 设A为需要训练的排序算法，在某一步迭代中，对于一个query 假设只有2个doc：D1，D2是相关的，然后模型A把D1排在了第一位，把D2排在了第n位，那么这一步的错误的对数有：n-2个，而WTA cost却为0；如果在下一步中，模型A的参数被调整了，然后这回它把D1排在了第2位，D2排在了第3位，此时WTA cost达到了最大：1，然而pairwise error却减小了n-4，到此为止，大家对两类代价函数不一致问题应该有了形象的理解。 下面说明对求导过程的干预。如果某一次迭代中，模型A把D1排在了第2位，把D2排在了第n位（n&gt;&gt;1），那么为了使得WTA的代价最小，我们应该更加关注D1，D1只需要前移一位就可以使得WTA的代价最小啦，形式化如下：$$|\\frac{\\partial C}{\\partial s_{j1}}|&gt;&gt;|\\frac{\\partial C}{\\partial s_{j2}}|$$ 也就是希望在doc1上的梯度要远大于doc2上的，而pairwise的error在此情况下通常会更加关注排在后边的doc2，这也是两类代价函数不一致带来的问题。 为了达成上述的目标，作者引入$\\lambda$函数来描述这种需求，形式如下：$$\\frac{\\partial C}{\\partial s_{j1}}=-\\lambda_j(s_1,l_1,···,s_{ni},l_{n_i})$$ 在这种形式下，$\\lambda$的具体形式可以根据任务的需求自定义，那么问题来了： 给定$\\lambda$函数，何等条件下C存在？ 如果C存在，何等条件下是凸函数？ 从力学角度分析$\\lambda$函数的约束针对上述两个问题，作者在论文中给出了详尽的证明，本人数学渣，在此只说明结论：$\\lambda$的Jacobian阵必须是半正定的。 文中还提到，LambdaRank可以很清晰的从力学的角度加以解释：把一个query下返回的docs看做是一群质点，那么$\\lambda_j$就是作用在质点上的力，如果$\\lambda$的Jacobian阵是半正定的，那么在模型中的力就是保守的（保守力的定义就是力沿闭合路径所做的功等于零），也就是说通过$\\lambda$建立的隐含的C函数可以看做是一种势函数。 在这种力学上的解释下，可以对于$\\lambda$函数的选择提出两点指导： $\\lambda$之和应该为0 根据牛顿第三定律，$\\pm\\lambda$总是成对出现，有没有发现，这点正好可以直接利用在pairwise的训练中！ $\\lambda$函数在ranknet的使用在ranknet中，一个pair的代价函数为：$$C^R_{i,j}=s_j-s_i+log(1+e^{s_i-s_j})$$把NDCG带入到上式的导数中，可以推出：$$\\lambda =N(\\frac{1}{1+e^{s_i-s_j}})(2^{l_i}-2^{l_j})(\\frac{1}{log(1+i)}-\\frac{1}{log(1+i)})$$更新参数时，对于每一对，在sort之后，再使用$\\pm\\lambda$分别作用于D1和D2.","tags":[{"name":"排序学习","slug":"排序学习","permalink":"https://coderskychen.cn/tags/排序学习/"}]},{"title":"《翅膀》","date":"2017-05-12T02:31:44.000Z","path":"2017/05/12/poetry-wings/","text":"其实你也有这样的翅膀。我有一颗珍珠就像你肩上的月亮我还有一双翅膀落在了你的眉梢 于是，我听见了时间在你的心上长草光芒从你的眼底流出","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"LSTM for Sentiment Analysis 挖坑笔记","date":"2017-05-11T11:35:51.000Z","path":"2017/05/11/lstm-sentiment/","text":"LSTM Networks for Sentiment Analysis是一篇教我们如何在theano上使用LSTM做影评情感分类的教程。其中LSTM在theano的实现十分经典，代码很难读，在学习了我所大神Physcal和onmyway321的博文后，对这个代码有了大体的理解，这里做一些梳理和补充，重点分析了其中高度并行的lstm框架。 标准的LSTM模型传统的RNN中，在误差反传的阶段，梯度在每一个时间步中都会与weight相乘，因此，weight阵对整个学习的过程有很大的影响，表现在两个方面 weight过小，则可能出现梯度消失问题 weight过大，可能导致学习过程难以收敛 在LSTM中提出了新的结构来解决RNN学习过程中的对之前信息的利用不足问题（遗忘），一个LSTM单元结构如下图，主要有四个部分： 输入门，根据新的信息计算候选细胞状态$\\widetilde C_t$，并求得$\\widetilde C_t$的接受程度$i_t$$$i_t=\\sigma(W_ix_t+U_ih_{t-1}+b_i)$$$$\\widetilde C_t=tanh(W_cx_t+U_ch_{t-1}+b_c)$$ 忘记门，计算上一步的$C_{t-1}$的被接受程度：$f_t$$$f_t=\\sigma(W_fx_t+U_fh_{t-1}+b_f)$$至此，可以计算出当前状态的C值了:$$C_t=i_t * \\widetilde C_t + f_t * C_{t-1}$$ 输出门，决定当前状态的结果对下一步的影响程度$$o_t=\\sigma(W_ox_t+U_oh_{t-1}+V_oC_t+b_o)$$至此，可以计算出当前步中LSTM单元的输出$h_t=o_t*tanh(C_t)$ 自连接，泛指神经元的输入指向自己的部分,本例中指h,C 情感分类任务的模型这个任务中用到的lstm模型是标准lstm的一个变种，表现在输出门的计算不考虑当前的细胞态，即输出门的公式变为了：$o_t=\\sigma(W_ox_t+U_oh_{t-1}+b_o)$，这样做之后，$o_t$与$i_t,f_t$不存在依赖关系，因而$i_t,f_t,o_t$可以并行计算（$\\widetilde C_t$也可以并行算，但是它本来就与$o_t$没有依赖）。 整体的框架如下图： 图中LSTM在时间上进行了展开，在这里“时间”具体指某一句话的单词个数，$x_t$即第t个单词 图中一个LSTM方块包含多个LSTM单元，在代码中是dim_proj个单元，所以本图中$h_t$的维度是dim_proj 所有时序的输出求平均，再做logistic回归，完成分类任务 数据预处理原始的数据一共有5w条影评，它被平分为了train和test集，并且正负样例是均衡的。 建立字典：统计train集合（而不是整个语料库）中所有词的词频，然后降序排列，那么字典结构：key=term，value=index+2（留出0,1给没有见过的词） 这个字典的作用就是相当于为每个词设定了一个唯一的id，然后就可以用id来表示这个词，这样做的目的是要节省空间，因为我们最终是要把一个单词以词向量的形式表达的，但是我们不需要把所有的训练数据都在这个阶段表示成词向量的形式，只需要维护一个词向量的字典，然后在需要的时候去查字典，获取需要的词向量。 数据格式：1234f = open('imdb.pkl', 'wb') pkl.dump((train_x, train_y), f, -1) pkl.dump((test_x, test_y), f, -1) f.close() 可以看出代码中train和test先后压入文件imdb.pkl 其中，train_x 是一个2-D list，即一行一句话，每一个元素是id(不是真正word)，train_y是一个1-D的label lstm.py的几点分析词向量在哪里？1234# embedding randn = numpy.random.rand(options['n_words'], options['dim_proj']) params['Wemb'] = (0.01 * randn).astype(config.floatX) #0-0.01 init 答案是：没有提供预训练好的词向量，它的词向量是随机初始化的，并且会在lstm学习的过程中，不断的调整(学习)随机初始化的词向量，所以情感分类任务完成之后，这个词向量字典就是它的一个副产品。 样本如何从id转化为词向量的？123emb = tparams[&apos;Wemb&apos;][x.flatten()].reshape([n_timesteps, n_samples, options[&apos;dim_proj&apos;]]) 就这一句话搞定。其中：tparams[&#39;Wemb&#39;]是一个2-D词向量矩阵:n_words*dim_proj;而x就是当前采样出来的句子集合，所以也是2-D矩阵:n_samples*n_timesteps，每一个元素是单词id，在这里的含义就是Wemb矩阵的第id行的意思。 所以，这行代码整句话的意思就是：根据x.flatten()依次从Wemb词向量矩阵中取出需要的词向量,然后按照顺序组成3-D矩阵:n_timesteps*n_samples*dim_proj 高度并行的lstm框架从两个角度进行并行计算三个门的并行计算稍加观察会发现在计算门状态的时候有一个相同的模式：$Wx+Uh+b$，同时三个门没有依赖关系，因此代码中把$W_i,W_c,W_f,W_o$按列拼成了$W$，$U$也是如此。but，有一个细节需要注意：W*x+b与U*h并行的粒度是不同的，前者能够在step、句子、单词三个级别上并行，而后者却不能，因为h在step的维度是串行的，而对于x我们可以提前知道它在所有step的取值。1234W = numpy.concatenate([ortho_weight(options['dim_proj']), ortho_weight(options['dim_proj']), ortho_weight(options['dim_proj']), ortho_weight(options['dim_proj'])], axis=1) 同时，在每一轮中($x_0$~$x_t$)，网络的参数是一样的,因此在每一轮开始时，我们就可以计算出这一轮所有时间步的值，具体来说：对于一句话x，假设其中有10个单词，每个单词用128维的词向量表示，那么x:[10,128]，这里的时间步就是10，普通的计算方法是：one step one word，以W*x为例，伪代码如下：1234#input:x:[10,128]for step in rang(10): for w in [wi,wc,wf,wo]: #w:[128,128] 第二维128是因为有128个lstm单元 wx = x[step].dot(w) #dot：矩阵乘法 上述的操作可以在一个矩阵乘法中完成，即：把四个小的w按列拼接为W，然后直接xW，在这个例子中，`x:[10,128],W:[128,1284]`。只是在用的时候需要对大结果切片，取出我们要的那部分。在串行程序中，两层for与拼接之后一次矩阵乘法的时间复杂度是一样的，但是在theano框架下，矩阵乘法是并行的，因此拼接后效率很高。minibatch的并行计算minibatch是梯度下降的方法中对随机梯度下降的方法与批量梯度下降法的一种折中。即每个batch计算一次loss，更新一次参数。 具体到本例，就是多句话同时计算，代码中batchsize=16，假设句子最大长度为n_step=10，一次batch的维度:x:[10,16,128](n_step,batchsize,dim_proj)，而W阵的维度:W:[128,128*4]，x*W三维矩阵与二维矩阵的乘法，结果[10,16,128*4]，在theano中大量并行。12state_below = (tensor.dot(state_below, tparams[_p(prefix, 'W')]) + tparams[_p(prefix, 'b')]) 因此x*W+b的含义即为：计算16个句子中每个单词下的x*w+b 大概就梳理到这里吧，对theano完全无感，主要是再看的15年一篇论文用到了theano下的lstm，搜来搜去搜到了这个情感分类的lstm，正好学习一下lstm代码。","tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://coderskychen.cn/tags/深度学习/"}]},{"title":"《火》","date":"2017-05-07T10:57:02.000Z","path":"2017/05/07/poetry-hope/","text":"心如浮萍，未经世事。我的希望像是炸裂的火焰带着滚滚热浪有吞灭一切的活力 我的希望像是撒上鲜血的灰烬有澎湃过后的余辉随时幻灭，随时卷土重来","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"《星》","date":"2017-05-07T10:33:32.000Z","path":"2017/05/07/poetry-star/","text":"关于星星，是否可以这样思考？流星划过夜空留下的是无数双微闭的眼睛 流星划过夜空流出的是黑夜的鲜血 流星划过夜空切开的是光明与黑暗的距离 黑暗像是一个熟睡的婴儿打扰美梦的都是这世上最深的恶意","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"Learning to Rank using Gradient Descent 阅读笔记&源码解读","date":"2017-05-06T10:05:05.000Z","path":"2017/05/06/ranknet/","text":"该论文是排序学习中pairwise方式的经典之作，它关注的是一对样本的相对位置。作者提出了一个使用梯度下降的方法来学习ranking function的这样一种框架。主要工作有两点，1.提出了一种概率损失函数。2.在浅层神经网络上实现了这个学习框架，即：RankNet。我认为比较重要的是概率损失函数的提出，有了这个损失函数，我们就可以在任意模型中来完成ranking function的学习，只要该模型的参数可导。 1.概率损失函数该论文是一种pairwise的排序学习，因此，它的学习目标是学习到一对样本的先后顺序。对于同一个query下的两个样本A、B，$\\overline P_{AB}$表示A排在B之前的概率，而我们要学习的ranking function是用来对某一个样本计算得分的，设$o_A=f(x_A),o_B=f(x_B)$，$o_A,o_B$为样本A、B的得分，我们只需要关注$o_A,o_B$的相对大小即可，如果$o_A&gt;o_B$，那么样本A就排在B前边。设$o_{ij}=f(x_i)-f(x_j)$来表示两个样本评分的差值，那么样本i排在样本j前边的概率就可以借助logistic函数形式化为$P_{ij}=\\frac{e^{o_{ij}}}{1+e^{o_{ij}}}$,并且设定相应的label为$\\overline P_{ij}$，使用交叉熵损失函数则有：$C_{ij}=-\\overline P_{ij}logP_{ij}-(1-\\overline P_{ij})log(1-P_{ij})$，进一步化简可得：$C_{ij}=-\\overline P_{ij}o_{ij}+log(1+e^{o_{ij}})$ 2.RankNet：概率损失函数在神经网络中的应用对于只有一个隐藏层的神经网络来说，输入输出可以写成如下形式：$$o_i=g^3(\\sum_jw^{32}_{ij}g^2(\\sum_kw^{21}_{jk}x_k+b^2_j)+b^3_i)$$其中，右上方的数字代表层数，右下方的字母代表相应层的第几个神经元，那么损失函数f对于参数的偏导数则：$$\\frac{\\partial f}{\\partial b^3_i}=\\frac{\\partial f}{\\partial o_i}\\frac{\\partial o_i}{b^3_i}=\\frac{\\partial f}{\\partial o_i}g^{‘3}\\equiv \\Delta^3_i$$$$\\frac{\\partial f}{\\partial w^{32}_{in}}=\\frac{\\partial f}{\\partial o_i}\\frac{\\partial o_i}{\\partial w^{32}_{in}}=\\frac{\\partial f}{\\partial o_i}g^{‘3}g^2_n=\\Delta^3_ig^2_n$$$$\\frac{\\partial f}{\\partial b^2_m}=(\\sum_i\\frac{\\partial f}{\\partial o_i}\\frac{\\partial o_i}{g^2})\\frac{\\partial g^2}{b^2_m}=(\\sum_i \\Delta^3_iw^{32}_{im})g^{‘2}\\equiv \\Delta^2_m$$ （需要注意求和项，这是因为output可能有多个节点） $$\\frac{\\partial f}{\\partial w^{21}_{mn}}=x_n\\Delta^2_m$$ 具体到RankNet中1.前馈两次，反传一次，即：每一对样本更新一次weights,这点有别于传统的训练：一次前馈一次反传。2.每一次更新时，$o_2、o_1$分别代表两个输出样本对应的输出值，那么损失函数为：$f(o_2-o_1)=C_{21}$（交叉熵损失函数）3.参数更新：$\\frac{\\partial f}{\\partial \\alpha}=(\\frac{\\partial o_2}{\\partial \\alpha}-\\frac{\\partial o_1}{\\partial \\alpha})f^{‘}$，其中$\\alpha$泛指模型中的参数，根据这个小公式我们可以很轻松的由之前推倒的偏导公式扩展为pairwise下的偏导公式。$$\\frac{\\partial f}{\\partial b^3}=f^{‘}(g^{‘3}_2-g^{‘3}_1)\\equiv \\Delta^3_2-\\Delta^3_1$$$$\\frac{\\partial f}{\\partial w^{32}_{m}}=\\Delta^3_2g^2_{2m}-\\Delta^3_1g^2_{1m}$$$$\\frac{\\partial f}{\\partial b^{2}_{m}}=\\Delta^3_2w^{32}_mg^{‘2}_{2m}-\\Delta^3_1w^{32}_mg^{‘2}_{1m}$$$$\\frac{\\partial f}{\\partial w^{21}_{mn}}=\\Delta^2_{2m}g^1_{2n}-\\Delta^2_{1m}g^1_{1n}$$ 3.带着问题读源码在github上找到了用pyhton实现的RankNet，传送门：RankNet-pyhton 1.训练数据是什么样子的？训练数据是query-doc对的集合，一个query-doc为一个样本，每一个样本的label为相关程度，三级【0,1,2】 2.如何从query-doc构建训练使用的pair？遵循一个原则：一个pair中的两个样本必须在同一个query下，这个很好理解，不同query下的query-doc对没有可比性。另外在这个实现中，做了一些简化：只把label不同的构建为一个pair。在实现时，pair[i,j]，默认$x_i$排在$x_j$前边（如果不是则调换顺序，使其满足），这样在计算loss时的ground truth 就始终是1了，即$\\overline P_{ij}=1$ 3.神经网络的结构与权重更新网络结构：46-20-1，输入46个节点，隐藏层20个节点，输出层1节点训练是以pair为单位的，对于pair[i,j]，样本$x_i，x_j$先后输入到网络进行前馈计算，随后根据两次的输出：$o_i,o_j$计算loss，loss反传更新权重。 4.每一轮训练误差的评价标准是什么？预测错误的pair个数/所有的pair","tags":[{"name":"排序学习","slug":"排序学习","permalink":"https://coderskychen.cn/tags/排序学习/"}]},{"title":"《来·去》","date":"2017-04-11T06:47:37.000Z","path":"2017/04/11/poetry-come-go/","text":"吃完午饭，回来的路上看到街边繁花盛开，又联想到了《嫌疑人》的情节，突生感慨。尊重原创，转载请注明出处。 你来的时候曾撑着流星的桅杆照亮过我的夜空你走的时候我满怀盛开送你半路芳香","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"《靠近》","date":"2017-03-22T10:13:54.000Z","path":"2017/03/22/poetry-closer/","text":"老夫诗性大发，附现代诗一首。 尊重原创，转载请注明出处。 有一种靠近不是因为厌倦了孤独这位老友也不是盲溺于世俗的引力旋涡 而是 你倾慕我的挺拔身姿也看得到我背后的晦暗潮湿我神往你的傲霜斗雪也看得到你眼底的落寞孤绝 除外，无他","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"再见，我的2016","date":"2017-01-26T02:27:31.000Z","path":"2017/01/26/farewell2016/","text":"我的2016，关于相聚、离散、朋友、成长。 农历2016年就要结束了这一年的主题词应该是：相聚离散，有所成长 6、7月份告别了我的大学，我的舍友还有cc、悦悦、老脖子、大烽旭良哥还好也是随时能见到接着送别了大师兄大师兄这一走，除了voln再也没人能压制欢神了哎，真怀念大师兄欺负欢神的样子哦，还有辉哥我觉得他是麦片粥小王子最后是豆芽，毕业后竟去部队了我知道他心中一直有一个军人梦奔跑在追梦路上就是最幸福的吧还有小北，说要回家发展 17年1月，姥姥猝然长逝使我愈发觉得自己的成长速度赶不上亲人老去的速度我要加速奔跑 9月在国科大结识了几个小哥哥py小王子，jly，大朱，顺儿，jack豪，pofei杰还认识了一个小姐姐他们身上的闪光点在一定程度上影响到了我感谢 2016年，我开始相信一切都是最好的安排。","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"第五名分享-搜狗用户画像挖掘-2016CCF大数据与计算智能大赛","date":"2016-12-27T20:48:54.000Z","path":"2016/12/28/【干货分享】2016CCF大数据与计算智能大赛-搜狗用户画像挖掘/","text":"本文主要介绍了我们(The Right团队)在2016CCF_BDCI搜狗用户画像挖掘赛题中的方案，PPT及代码已公开。阅读本篇大约需要10分钟。 尊重原创，转载请注明出处。终于结束了两个多月的长跑，复赛B榜第五的成绩很知足。实验室是做图像的，我对NLP这块的了解很有限，从国庆到12月底，一直不断摸索。一路走来，感恩这个比赛和DF平台使我收获了成就感与宝贵经历；感恩队友，给我不同角度的建议，开阔了我的思路。下边是复赛B榜成绩： 赛题简述业务背景：用户画像，即：用户的属性及偏好。在广告的精准投放中，根据用户的历史行为来反推用户的属性是一项基础技术。本次赛题即为根据搜狗用户在一段时间内的搜索词来预测用户的性别、年龄和教育水平 复赛训练数据：10w个用户在某段时间内的搜索词以及相应的属性：性别、年龄和教育水平，注意有缺失值。 复赛预测数据：10w个用户在某段时间的搜索词。 性别、年龄和教育水平分别有：2、6、6个标签。 方案概述首先对用户查询文本进行分词处理，然后从特征词与词向量这两个方面提取文本特征，最后结合集成模型stacking完成三个属性的训练与预测。 主要算法改进1.基于布尔模型的S-TFIWF(Supervised TFIWF)1.1 经典特征加权：TFIDF算法TFIDF算法是经典的文本特征加权方法，它衡量了某一个单词在文档中的重要性。 TFIDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反相关。下面是TFIDF算法的公式： $$ TFIDF(w,d)=TF(w,d) * log(\\frac{N}{DF(w)}) $$ \\( TF(w,d) \\): 词语w在文章d中的词语频率 \\( N \\)：语料库中的文档总数 \\( DF(w) \\): 训练语料中包含单词w的文档数 1.2 TFIDF算法的优劣TFIDF算法的优势： 综合考虑了单词在文档中的词频和单词在整体语料库中分布的影响。 TFIDF算法的不足： 单词在文档中的出现次数TF对于整体权重影响过大。 没有考虑到单词在不同类间的分布差异。 1.3 改进算法：基于布尔模型的S-TFIWF针对TFIDF的不足，我们认为用于文本分类的特征权重应该考虑以下内容： 抑制TF对整体权重的影响-布尔模型：只关心单词在文档中出现与否。 引入单词在不同类间的分布差异，使用归一化的方差进行量化。 我们提出如下公式： $$ S-TFIWF(w,d)=TF(w,d) * log(\\frac{N}{WF(w)}) * \\sqrt{\\sqrt{\\sum_j (p_{wj} - \\overline{p_w} ) ^ 2}/ \\sum_j p_{wj}} $$ \\( TF(w,d) \\) : 单词w在文章d中出现与否 \\( N \\) ：训练语料中所有词出现次数之和 \\( WF(w) \\) : 训练语料中出现单词w的次数 \\( p_{wj} \\) : 单词w在类j中出现的概率 \\( p_w \\) ：单词w在所有类中出现的平均概率 对公式的解释： \\( TF(w,d) \\).在布尔模型下，当单词w出现在文档d中时𝑇𝐹(𝑤,𝑑)即为1，反之为0。 \\( log(\\frac{N}{WF(w)}) \\) 当单词在整体语料中出现多次时，该单词可能是常用词，对于类别判断帮助小，应该抑制。 \\( \\sqrt{\\sqrt{\\sum_j (p_{wj} - \\overline{p_w} ) ^ 2}/ \\sum_j p_{wj}} \\) 该项利用了单词在不同类间的概率方差，代表了单词w的类间分布信息。通过这个信息，我们可以判断，当某个单词在不同类中的方差很大时，这个单词更具有区分度，权重应该更大。 改进前后对比 算法 线上结果 TFIDF 0.696 S-TFIWF 0.699 2.模型集成：Stacking2.1 单分类器、Bagging与Boosting分析 单分类器模型。缺点：对训练数据利用不足，过拟合风险很大。 Bagging与Boosting均属于集成方法：其中，Bagging特点：对数据随机采样N次，采样后的数据集在训练时是相互独立的，因而可以并行；Boosting特点：在迭代中对改变数据样本的权重，迭代之间有依赖关系，难以并行。就我目前的了解来看，这两类集成方法均不方便在中间结果融合新特征。 2.2 Stacking集成相比较来看，Stacking集成最大的特点是灵活，我们可以设置多层级的Stack，每层可以设置合适的分类器簇，并且可以将新特征很方便的融合在层之间的中间结果里。 Stacking，直译：堆积。简言之，经典的Stacking框架分为2层，第一层含有T个分类器，产生T组与原数据集规模相同且维度为1的结果，将这T组结果拼在一起可组成新的数据集，用以构成第二层的输入。这样说很抽象，请看下图： 其中，刚刚接触模型集成的同学可能不太清楚Stacking第一层中的做法。具体是这样的：对于T个分类器中的每一个分类器，我们把训练数据分割为N份，利用其中的N-1份做训练，剩下的那一份做预测（类似N折交叉验证，这里N一般为5），这样对训练集重复N次，就可以得到在一个分类器下对原始训练数据的一个完整预测结果（可以称为新表达），于是一个分类器可以得到 \\( N*1 \\) 的新表达，那么T个分类器就可以得到 \\( N*T \\) 的新表达，而这就是用于下一层的输入。需要注意的一点是，我们需要保证第二层的训练集与测试集具有相同的模式，因此对原始测试集的处理略有不同：第一层的每一个分类器的每折训练都要对整体原始测试集进行预测，于是一个分类器会得到N个测试集的预测结果，而我们的目的是一个分类器获得一份测试集的预测结果，这里我们采用的是对N个结果求平均的方法。 stacking框架强大的地方在于我们可以在第一层中选择许多不同的分类器，而且在第二层中还可以根据具体问题添加特征（word2vec特征就是在这里融合的），甚至还可以把整个框架扩展为3层或更多，我们试过3层的效果并不理想。我们最终方案中，第一层有25个模型（从sklearn中选择了几种不同的分类器，每种分类器设置几种不同的参数），第二层只有一个SVM。 2.3 结果对比 模型 线上结果 单分类器模型（线性核SVM） 0.6657 stacking模型 0.6810 3.融合特征：词向量3.1 word2vec简介词向量研究的是如何把单词表示为实数向量。word2vec是其中的一种训练方法，它根据单词的上下文信息获得单词的分布式表达（Distributed Representation），word2vec（以下简称wv）是一个无监督的方法，这个比赛中，我们喜爱无监督的方法，因为可以把测试集的无标注数据利用上，在一定程度上缓解标签缺失带来的不利影响。 3.2 word2vec的应用第一个问题：根据词向量，该如何表达一篇文档呢？假设某文档中含有N个单词，每个单词使用K维的词向量表示。有人可能会说，把词向量拼起来，将一个文档形式化为\\( N*K \\)的向量不就ok了吗？这个策略本身没有问题，但是在一般的情况下（非一般情况，比如卷积神经网络，利用pooling层可以解决维度不一致问题）这种做法并不合适，因为 无法解决不同的文档长度所导致的维度不一致问题 。因此，我们采用了如下方法，把单词向量加和求平均，这种做法是有道理的：如果某一个单词表达能力越强，加和求平均得到的向量就越偏向该单词向量所指方向。 第二个问题：得到了文档在wv下的向量表达，又该如何使用呢？ 首先，我们认为把K维的WV向量直接拼接在S-TFIWF特征上不是一个好的策略，因为S-TFIWF特征具有高稀疏度、高维度的特点，与WV这种稠密的形式区别过大。回顾上文中介绍到的Stacking集成框架，我们把WV特征拼接在了stack的第二层中，举个栗子：如果第一层有20个分类器，WV的维度为K，那么第二层特征的维度就变为了：\\( K+20 \\)。 S-TFIWF+WV从特征词与词向量这两个角度完成了文档的表达。 3.3 改进前后对比 做法 线上结果 未融合WV 0.6993 融合WV 0.7049 最后经过参数调优，在A榜以0.71172的成绩排在第五名。 4.尝试过的方法分析4.1 一元词与n-gram的组合 动机：n-gram在一定程度上考虑了词组语义，可能有更强的表达能力 做法：把一元词与2-gram词组合起来，作为特征字典 效果与反思：线上不理想。可能因为我们没有找到更好的组合方式。 4.2 特征词加权改进：SS-TFIWF (SS:smooth-supervised) 动机：尝试利用词频信息，同时通过平滑措施，使词频影响得到抑制。 做法：在VSM模型下，对于词频项，我们使用tf/(tf+0.2)进行平滑,其他项与S-TFIWF相同。 效果与反思：本地测试有提升，线上比布尔模型的S-TFIWF效果有略微下降（0.7090 vs 0.70914）。 4.3 LDA主题特征提取 动机：相对于W2V特征，我们也提取过LDA主题特征，期待从更多的角度来增强特征表达 做法：利用gensim工具包，训练并获取文档的主题分布向量，以此作为特征，融入到stacking模型的第二层中。 效果与反思：相比于纯S-TFIWF特征有提升，但是不如W2V向量的融合效果好。 4.4 针对样本不均衡问题：进行欠/过采样 动机：样本不均衡会影响分类器的性能 做法：利用imbalanced-learn工具包，进行过采样，参数尝试过：线性、SVM。其中，在确定不同类的过采样比例时，我们根据2015年中国网民结构调查报告获取网民的先验分布，以此比例作为过采样比例。 效果与反思：线上不理想。发现该做法其实陷入了一个误区，我们不应该强行改变原始样本的分布情况，因为基于独立同分布的假设会使得分类器学到的分布也随之改变，最明显的表现就是：预测的博士人数过多，而这明显不合理。 4.5 缺失标签的填补 动机：填补缺失的标签，会增加可以利用的训练集规模，增强分类性能 做法：使用训练好的模型对缺失的标签进行预测，填补训练集。然后基于新的训练集完成比赛任务。 效果与反思：线上结果下降。由于分类器不可能完全做对，因此预测得到的标签有一部分是错误的，可能这些错误的样本使得重新训练的分类器性能下降。 4.6 使用卷积神经网络进行文本分类 动机：卷积神经网络在计算机视觉领域有了成功的应用。 做法：使用2014年的TEXT-CNN，输入为分词后的文件和label。 效果与反思：初赛线上达到了0.64，但是限于设备条件，没有继续研究。但是可以看到CNN的强大，我们测试的CNN网络只有一个卷积层，几乎没有调参就跑出了0.64的成绩。 5.最后总结5.1 使用TFIDF权重时的特征维度问题其实我们使用的S-TFIWF是TIFDF的一个变种，是在特征加权上的优化，但是在特征选择问题上与TFIDF是没有本质区别的。一个很奇怪的问题是：我们尝试过卡方、信息增益、互信息等方法做特征词的筛选，但是从线上结果来看，使用简单的词频筛选效果更好，对此我们也没有更好的解释。 5.2 深度学习的威力在比赛中期间我一直很好奇第一名的方案是怎么做的，因为人家线上超了我们1.5个百分点，在10w的测试集规模下，这绝对是本质的差别。 终于，在决赛领略到了高手的风采！前四名均在一定程度上的结合了神经网络(BPNN/CNN)，这样做的一大好处就是可以通过设置神经网络的共享层学到年龄、学历这两种属性的关联，避免了人工设计特征。而我们的模型是没有考虑属性间的关联的，我们曾想过这个问题，但是没有解决。 5.3 两点细节决赛第一名：大大黑楼，有两点做得很细腻 如果三个属性中至少有一个属性缺失，那么就放弃该样本，因为他们认为这样的样本是不可靠的。 提取TFIDF特征时，保留单个查询中的空格。 5.4 赛题难点搜狗大数据技术总监高君老师在总结时提到： 训练集部分标签缺失。这是为了模拟真实的业务情况，增加了难度。 训练样本质量。想象一种环境：在家庭中，多人共用一个搜狗账号进行搜索，这会导致搜索文本的内容与用户属性有偏差，数据中的噪声很大。 5.5 干货地址PPT及源码 先总结到这，爆照一张","tags":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://coderskychen.cn/tags/数据挖掘/"},{"name":"文本分类","slug":"文本分类","permalink":"https://coderskychen.cn/tags/文本分类/"},{"name":"python","slug":"python","permalink":"https://coderskychen.cn/tags/python/"}]},{"title":"附上七言绝句一首","date":"2016-10-23T08:16:39.000Z","path":"2016/10/23/半夜睡不着，附上七言绝句一首/","text":"《孤》尊重原创，转载请注明出处。 &lt;&lt;孤&gt;&gt;寒风染霜雨后笛始觉归途行人稀同是红尘漂泊客莫问前程莫行急朋友看了这首绝句之后，随笔和我对了一首。最懂我的，果然是兄弟们。&lt;&lt;无题&gt;&gt;作：心恒他人终是匆匆客唯有孤独伴终身莫说红尘戏众生莫负少侠好武功","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"十一劳动节-记大数据类比赛的第一次提交","date":"2016-10-03T13:59:39.000Z","path":"2016/10/03/大数据类比赛第一次提交/","text":"小小的纪念~ 从零基础到baseline的诞生一开始我没打算参加这个比赛（因为没听说过这个比赛。。。是我孤陋寡闻了吧），但是高级人工智能课程要求我们以这个比赛作为课程设计，于是就和隔壁组的小伙伴(小p)组队了。有趣的是小p和我都是做图像的人，小p建议我们做搜狗用户挖掘，于是，我们两个做图像的人走到了一起，加上小p深交多年的基友，开启了文本挖掘的征途。 先说一下我们的队名：The Right（因为我们一致认为脱单是首要的。难道不是吗？_？）。 十月一日数据公布，队长小p去找女朋友了，颖哥回家了，我带着笔记本也回家了，我当时打算先在国庆期间做出一个baseline，等开学再交流。小p确定编程以python为主，后来也的确体会到了python的便捷。当时我对py知之甚少，不过还好，程序员最擅长的就是自学。于是边学边用，很快就能上手了。放假前和小p商量，我们初步把赛题看待为三个分类问题，一个属性对应一个分类问题。 用时三天完成了汉字编解码+jieba分词+随机特征词选择+手写tfidf加权+朴素贝叶斯分类器，在第三天晚上获得了第一份可以提交的结果！当时真的很激动，赶紧提交了一波，11/16，没有倒一我就知足啦~ 国庆后几天小p使用sklearn的包做tfidf加权和特征选择之后我们的结果直接提到了0.60+，后来和小p交流后我才知道，原来sklearn如此强大，包含了预处理、TFIDF特征提取、特征选择降维以及各种分类器和集成算法（而我还傻傻的自己写tfidf），从这我发现对于网络资源的利用以及对sklearn的了解还远远不够。 国庆大概就做了这些事情，我觉得最重要的是在这期间明白了文本分类的大体流程，尤其是文本向量化的方法，为后续的提升打下了基础。","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"第一次被推首页-纪念一下","date":"2016-09-27T14:27:28.000Z","path":"2016/09/27/第一次被推首页-纪念一下/","text":"小小的纪念~ 今天有人从qq加我好友，备注是：opencv编程，我第一感觉应该是有人在csdn留言了吧，正好突然发现有一段时间没登录csdn了，下课后果断登录一波，结果竟然发现去年写的一篇暗通道去雾的博文被推荐到首页了，作为小硕的我感觉还是相当激动呢！纪念一下~","tags":[{"name":"随笔","slug":"随笔","permalink":"https://coderskychen.cn/tags/随笔/"}]},{"title":"softmax函数与多项分布","date":"2016-09-18T07:09:58.000Z","path":"2016/09/18/softmax函数与多项分布/","text":"摘要：从神经科学的角度来看是很有意思的，我们可以认为softmax函数为输出单元创建了一种竞争模式：由于softmax的输出值之和为1，因此，如果有一个输出单元的值增大了，那么其他所有单元的值都将会受到抑制！而这与生物皮质神经元之间的侧向抑制现象不谋而合啊！在极端的条件下，这就会演化为一种胜者通吃的模式！(winner-take-all)尊重原创，转载请注明出处。 当我们希望表示有n种可能的离散变量的概率分布情况时，softmax函数就隆重登场了。它可以被看为是sigmoid函数的一种推广。 Softmax函数的身影常出没于多分类任务的分类器中，但是也有一种不太常见的应用，就是当我们希望模型从多个不同的的值中进行选择的时候，softmax函数可以被用于模型内部变量的确定。 我们回顾之前的二分类案例，我们希望产生一个数值： 因为这个数值应处于0到1之间，而且我们想要这个数值的对数能够在基于log似然的损失函数的梯度下降算法中有很好的性能，我们选择了预测一个数值z=logP(y=1|x)作为替代。通过指数化与标准化，我们得到了一个受到sigmoid函数控制的伯努利分布。 现在来看看更一般的情况，对于有n个可能的离散变量，我们现在需要得到一个向量y，其中yi=P(y=i|x)。我们不仅要求每一个分量yi处于0到1之间，还要求分量之和为1，这样才是一个有效的概率分布。我们还是以分析伯努利分布的方式来分析多项分布。 首先有一个线性层预测尚未归一化的log概率： 其中，zi=logP(y=i|x)。随后softmax函数将其指数化、归一化就得到了我们希望的y。正式地讲，softmax函数形式如下： 我们希望最大化logP(y=i|x)=log softmax(z)i. 我们之所以将其对数化，原因还是一个：好计算~(因为log可以抵消exp) 6.30的第一项表明输入zi对损失函数有直接的影响，因为该项不会饱和所以我们可以断定基于6.30的梯度下降法能够高效的完成(即使第二项饱和也没有关系哦)。当最大化log似然时，第一项zi会趋于增大，而第二项则使得z的所有分量降低；对于第二项，容易发现如果z的某一个分量远大于其他项，那么第二项就会趋向于那个分量的值；因而在迭代的过程中，算法会对误判的样例进行更强的惩罚！ 除了log似然，其他目标函数与softmax并不那么搭，还是因为前边层多次提到的梯度下降速度的问题。为了理解为何其他函数会失效，我们需要仔细审视一下softmax函数本身。 我们先给出结论：类似sigmoid，softmax也有近饱和的问题，在softmax函数中，存在多个输出值，当输入值之间的差别非常大的时候，它的输出值就会近饱和。 好，接下来我们来分析一下这个结论是如何得出的： 经过仔细观察与推导，我们很容易得到一下结论： 这个式子的意思就是如果softmax函数的所有变量同时增加或减少同一个标量的话，函数值是不变的！（是不是很神奇0.0 不放心的同学可以自己推一下，其实就是分子分母同时提取了一个exp(c)） 根据上述性质，我们紧接着就会得到如下的式子： 推到这里寓意就很明显了，当输入值之间的差别非常大的时候，它们的输出值几乎不会变化。这与我们上边的结论首尾呼应！而这个结论其实与sigmoid函数饱和状态的一个推广形式！ 从神经科学的角度来看是很有意思的，我们可以认为softmax函数为输出单元创建了一种竞争模式：由于softmax的输出值之和为1，因此，如果有一个输出单元的值增大了，那么其他所有单元的值都将会受到抑制！而这与生物皮质神经元之间的侧向抑制现象不谋而合啊！在极端的条件下，这就会演化为一种胜者通吃的模式！(winner-take-all)","tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://coderskychen.cn/tags/深度学习/"},{"name":"翻译","slug":"翻译","permalink":"https://coderskychen.cn/tags/翻译/"}]},{"title":"helloworld","date":"2016-09-18T03:52:02.000Z","path":"2016/09/18/helloworld/","text":"hello 这是我的第一个自建博客~","tags":[{"name":"test","slug":"test","permalink":"https://coderskychen.cn/tags/test/"}]},{"title":"暗通道去雾算法的C++实现与优化（三）","date":"2016-01-08T14:00:33.000Z","path":"2016/01/08/暗通道去雾算法的C-实现与优化（三）/","text":"本文主要介绍了暗通道去雾的优化策略。尊重原创，转载请注明出处。 点我查看测试样例一、原论文方法的局限性谈到优化，我们首先需要了解原论文的方法有何局限，在实际编码测试中我发现了以下的三点局限： 1.太过耗时在上一篇中，在透射图的精细化（refine）中原论文使用的是softmatting方法，而我使用的是引导滤波，一是因为softmatting我看不太懂（囧。。）二是因为大家都说softmatting实在是太耗时间了，而引导滤波的时间复杂度是常数并且其效果不比softmatting差。即使在使用引导滤波代替原文softmatting之后，处理一幅324*284的图像也需要150ms左右，这对于视频处理来说时间还是太长了（一般要求30ms以内）！ 2.色斑，色块效应在实验中发现，当像素点的强度（intensity）接近大气光值（airlight）时，去雾的图像会出现局部的色斑，色偏效应，这样说的比较抽象，大家可以看一下下边的对比图： 大家可以看到左图上部区域出现了明显的色斑效应，这是因为上部分的图像像素强度非常接近大气光值，有人可能会问，为什么接近大气光值就会导致色斑呢？ 还记得第一篇中提到的透射图的计算公式吗？如下： $$\\hat{t}(x)=1-w\\min_c \\min_{y\\in{\\Omega(x)}}\\frac{I^c(y)}{A^c} $$ $$J(x)=\\frac{I(x)-A}{max(t(x),t_0)}+A$$ 当I(y)非常接近A时，t(x)就会非常小，接近于0，也就是说t(x)&lt;t0，于是乎，当我们计算J(x)时，会有很多的像素对应着同一个t值，即t0，这就会导致出项色斑啦！ 3.图像亮度降低何在论文中也提到了，处理完之后，图像的亮度会有一点变暗，至于为啥，我暂时不能理解；这就需要我们使用一定的策略进行图像增强，我使用的是局部非线性增强的方法 二、解决的策略1.图像降采样处理这一点要特别感谢这位博主，受到他的启发； 我们知道：去雾的过程中计算的暗通道和透射率图像并不需要特别的高质量（高分辨率），那么我们就可以把图像降采样，把缩小的图像作为输入计算暗通道和透射率图像，在最后一步时我们再使用插值的方式resize暗通道和透射率图像到原图的大小，这样从理论上来说对效果没有明显的影响。 基于这样的思路，我进行了实验，其中我的降采样率为0.5，理论上来说应该可以减少1/4左右的计算量，效果如下： 原图： 降采样处理过的去雾效果图： 原图的去雾效果： 耗时对比 ：降采样处理过的去雾耗时：42ms；未处理的去雾效果：150ms 从上边的对比图像中可以发现，从主观上来看，降采样处理之后的效果几乎与未降采样的效果是一样的，但是，时间确大大减少了，仅仅需要42ms 啊亲！ 需要注意：不能把scale降采样率设置的太小了，一是会影响效果；二是在缩放图像过程中耗时可能会过多，得不偿失啊亲！ 2.优化t(x)的计算既然是因为计算t的时候出现的问题，那么我们可以人为的在t过小的时候进行一些处理，使它不至于总是比t0小； 12345678910111213141516for(int k=0;k&lt;nr;k++)&#123; const uchar* inData=darkimg.ptr&lt;uchar&gt;(k); const uchar* srcData=srcimg.ptr&lt;uchar&gt;(k); float* outData=transmission.ptr&lt;float&gt;(k); for(int l=0;l&lt;nl;l++) &#123; pix[0]=*srcData++; pix[1]=*srcData++; pix[2]=*srcData++; r=fabs((pix[0]+pix[1]+pix[2])/3.0-avg_A); if(r&lt;r0) *outData++=1-w*(*inData++/avg_A)+(r0-r)/r0; else *outData++=1-w*(*inData++/avg_A); &#125; &#125; 上述代码中，我加了一个判断，当像素的强度非常接近大气光值时，对于t的计算加上一个增幅项，即：（r0-r）/r0；其中，r是当前的像素intensity，r0是判断的阈值。 经过这样简单的处理之后，我们再来看看效果吧！见下图： 原图： 原始方法的去雾效果： 优化之后的效果： 对比可以看出来，优化之后明显的去除了第二幅图中的色斑效应！当然，阈值的选择很重要，太大了，图像去雾不明显，太小了色斑又会出来，大家可以继续优化，搞个动态阈值什么的~","tags":[{"name":"暗通道去雾","slug":"暗通道去雾","permalink":"https://coderskychen.cn/tags/暗通道去雾/"},{"name":"C\\C++","slug":"C-C","permalink":"https://coderskychen.cn/tags/C-C/"},{"name":"opencv","slug":"opencv","permalink":"https://coderskychen.cn/tags/opencv/"}]},{"title":"暗通道去雾算法的C++实现与优化（二）","date":"2015-12-11T06:48:48.000Z","path":"2015/12/11/暗通道去雾算法的C-实现与优化（二）/","text":"本文主要介绍了暗通道去雾的opencv实现。尊重原创，转载请注明出处。 上一篇中，学习了何的论文中的去雾方法，这一篇中，我按照何的论文思路借助OpenCV 2.4.10 进行了实现，效果的确很好，就是耗时太多了，效果见下图：蓝色圆圈代表大气光值的取值点 点我查看测试样例突然发现上一篇中忘了介绍大气光值A的求解了，论文中是这样做的： 1.首先取暗通道图中最亮的千分之一的像素点。 2.根据这些像素点的位置在原图中搜索一个最亮的点，这个点的强度（intensity）就是我们要求的A啦。 何认为这样做的好处就是避免了原图中比较亮的物体作为A的值，比如图片中的白色的汽车，如果从原图中搜索最亮的点不一定能搜到真实的大气光值，可能是白色物体的反射光 正式开始本文首先分享一下我在实现的过程中发现的几点： 1.大气光值A的解释，原论文中说那个最亮的点的强度就是我们要找的A，那么问题来了，最亮的点的强度是啥？大气光值A是一个值还是一个RGB三元组呢？ 可以肯定的是，A是RGB的一个三元组，RGB三个通道有各自的A，这一点可以从论文的公式中确认。那么搜索的时候，如何定义一个点的强度呢？这里，我采用的是求RGB的平均值。 2.透射率t(X)不是一个值，是一个同原图一样大小的矩阵，不同的位置有不同的透射率t 在具体求解的时候我进行了一点点简化，即：直接把暗通道图输入，这样减少了一次最小值滤波的过程，而且对结果几乎没有影响的，小小的提速。 3.透射率图片的精细化 这里我采用了别人写的引导滤波算法，这是一种边缘保持算法；实现的时候需要注意的是引导图需要实现归一化为0.0-1.0之间，因为t在0-1之间。附连接：引导滤波算法 4.最后求J的时候注意设置0-255的门限，因为按照公式求时可能出现很大的值也可能有负值出现！！！（调了半天才发现J可能为负值啊亲！） 5.opencv中遍历图片，最好要用指针啊亲！使用at随机取元素 和使用指针取元素在我的测试中相差了0.5s呢。 6.网上流传的一个opencv的c++代码中，在计算暗通道的最小值滤波时把原图分成了若干个window，每一个window赋予相同的值，这显然是不符合论文公式的，希望大家不要被误导！！可能那个博主是想降低时间复杂度吧，但是那样的操作太粗糙了，违背了滤波的概念。 暗通道的计算我的想法是，先求出每个像素中最暗的那个通道值，最后再统一进行最小值滤波。minfliter是最小值滤波函数，自己写一个就好。 12345678910111213141516171819202122232425262728293031Mat Producedarkimg(Mat&amp; I,int windowsize)&#123;int min=255;Mat dark_img(I.rows,I.cols,CV_8UC1);int radius=(windowsize-1)/2;int nr= I.rows; // number of rows int nl=I.cols;int b,g,r;if (I.isContinuous()) &#123; nl = nr * nl; nr = 1;&#125;for(int i=0;i&lt;nr;i++)&#123; const uchar* inData=I.ptr&lt;uchar&gt;(i); uchar* outData=dark_img.ptr&lt;uchar&gt;(i); for(int j=0;j&lt;nl;j++) &#123; b=*inData++; g=*inData++; r=*inData++; min=min&gt;b?b:min; min=min&gt;g?g:min; min=min&gt;r?r:min; *outData++=min; min=255; &#125;&#125;dark_img=minfliter(dark_img,windowsize); return dark_img;&#125; 计算大气光值A(airlight) 输入：暗通道图，原图，窗口大小(必须奇数） 输出：大气光值A，一个包含三个元素的一维数组头 其中，Pixel是我定义的结构体，定义在下边 注意c++返回数组类型时，一定要用new为数组分配空间，不能用int A[3]={0,0,0};这种方式！！否则返回的数组可能会被释放掉！（别问我为什么知道这么多，都是我爬过的坑啊） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647int* getatmospheric_light(Mat&amp; darkimg,Mat&amp; srcimg,int windowsize) &#123; int radius=(windowsize-1)/2; int nr=darkimg.rows,nl=darkimg.cols; int darksize=nr*nl; int topsize=darksize/1000; int *A=new int[3]; int sum[3]=&#123;0,0,0&#125;; Pixel *toppixels,*allpixels; toppixels=new Pixel[topsize]; allpixels=new Pixel[darksize]; for(int i=0;i&lt;nr;i++)&#123; const uchar* outData=darkimg.ptr&lt;uchar&gt;(i); for(int j=0;j&lt;nl;j++) &#123; allpixels[i*nl+j].value=*outData++; allpixels[i*nl+j].x=i; allpixels[i*nl+j].y=j; &#125; &#125; //std::qsort(allpixels,darksize,sizeof(Pixel),qcmp); std::sort(allpixels,allpixels+darksize,cmp); memcpy(toppixels,allpixels,(topsize)*sizeof(Pixel)); //找到了在darkimg中最亮的0.1%个 int val0,val1,val2,avg,max=0,maxi,maxj,x,y; for(int i=0;i&lt;topsize;i++) &#123; x=allpixels[i].x;y=allpixels[i].y; const uchar* outData=srcimg.ptr&lt;uchar&gt;(x); outData+=3*y; val0=*outData++; val1=*outData++; val2=*outData++; avg=(val0+val1+val2)/3; if(max&lt;avg)&#123;max=avg;maxi=x;maxj=y;&#125; &#125; for(int i=0;i&lt;3;i++) &#123; A[i]=srcimg.at&lt;Vec3b&gt;(maxi,maxj)[i]; //A[i]=srcimg.at&lt;Vec4b&gt;(maxi,maxj)[i]; //A[i]=A[i]&gt;220?220:A[i]; &#125; return A; &#125; 结构体： 123456typedef struct Pixel&#123; int x; int y; int value;&#125;Pixel; 计算透射图(transmission)并精细(refine)化 输入：原图，暗通道图，大气光值A，窗口大小 输出：透射图t 之所以需要暗通道图参数，是因为我进行了简化，使用暗通道图加速计算最后的引导滤波使用原图的灰度图进行引导，具体实现参考上述链接。 123456789101112131415161718192021222324252627282930Mat getTransmission_dark(Mat&amp; srcimg,Mat&amp; darkimg,int *array,int windowsize) &#123; float test; float avg_A=(array[0]+array[1]+array[2])/3.0; float w=0.95; int radius=(windowsize-1)/2; int nr=srcimg.rows,nl=srcimg.cols; Mat transmission(nr,nl,CV_32FC1); for(int k=0;k&lt;nr;k++)&#123; const uchar* inData=darkimg.ptr&lt;uchar&gt;(k); for(int l=0;l&lt;nl;l++) &#123; transmission.at&lt;float&gt;(k,l)=1-w*(*inData++/avg_A); &#125; &#125; Mat trans(nr,nl,CV_32FC1); Mat graymat(nr,nl,CV_8UC1); Mat graymat_32F(nr,nl,CV_32FC1); cvtColor(srcimg,graymat, CV_BGR2GRAY); for(int i=0;i&lt;nr;i++)&#123; const uchar* inData=graymat.ptr&lt;uchar&gt;(i); for(int j=0;j&lt;nl;j++) graymat_32F.at&lt;float&gt;(i,j)=*inData++/255.0; &#125; guidedFilter(transmission,graymat_32F,trans,6*windowsize,0.001); //bilateralFilter(transmission,trans,10,30,100); //GaussianBlur(transmission,trans,Size(11,11),0,0); return trans; &#125; 计算J(X) 输入：原图，透射图，大气光值，窗口 输出：去雾图 1234567891011121314151617181920212223242526272829Mat recover(Mat&amp; srcimg,Mat&amp; t,int *array,int windowsize)&#123; int test; int radius=(windowsize-1)/2; int nr=srcimg.rows,nl=srcimg.cols; float tnow=t.at&lt;float&gt;(radius,radius); float t0=0.1; Mat finalimg=Mat::zeros(nr,nl,CV_8UC3); int val=0; for(int i=0;i&lt;3;i++)&#123; for(int k=radius;k&lt;nr-radius;k++)&#123; const float* inData=t.ptr&lt;float&gt;(k); inData+=radius; const uchar* srcData=srcimg.ptr&lt;uchar&gt;(k); srcData+=radius*3+i; uchar* outData=finalimg.ptr&lt;uchar&gt;(k); outData+=radius*3+i; for(int l=radius;l&lt;nl-radius;l++) &#123; tnow=*inData++; tnow=tnow&gt;t0?tnow:t0; val=(int)((*srcData-array[i])/tnow+array[i]); srcData+=3; val=val&lt;0?0:val; *outData=val&gt;255?255:val; outData+=3; &#125; &#125; &#125; return finalimg;&#125; 几个效果示例 美丽天安门的透射图（transmission）如下：可以看到效果的确很好，很精细，这就是使用原图像的灰度图进行引导的好处。","tags":[{"name":"暗通道去雾","slug":"暗通道去雾","permalink":"https://coderskychen.cn/tags/暗通道去雾/"},{"name":"C\\C++","slug":"C-C","permalink":"https://coderskychen.cn/tags/C-C/"},{"name":"opencv","slug":"opencv","permalink":"https://coderskychen.cn/tags/opencv/"}]},{"title":"暗通道去雾算法的C++实现与优化（一）","date":"2015-12-11T02:08:37.000Z","path":"2015/12/11/暗通道去雾算法的C-实现与优化（一）/","text":"本文主要介绍了暗通道去雾的原理。尊重原创，转载请注明出处。 经过老师推荐，拜读了何恺明博士的暗通道去雾论文《Single Image Haze Removal Using Dark Channel Prior》，这是何发表的第一篇论文，令人惊艳的是该论文一举获得了2009年CVPR的Best Paper奖，三个审稿人都给了何最高分！何的这篇论文以简单，效果近乎完美而出名，主要问题是比较费时，后续有很多人提出了加速的改进，把时间控制在了ms级，从而实现视频去雾；废话不多说了，先来欣赏一下何的论文讲了些啥吧！ 点我看测试样例一 摘要大意：仔细想想我们可以确定有这样一个统计规律：对于大多数没有雾的图像来说，它的任意一个像素点中的R，G，B值至少有一个是非常低的；（这个挺好理解的，如果R,G,B值都偏高，那么该像素显然有向白色过度的趋势）把每个像素中“偏暗”的值（通道）以一定的方式集合起来就构成了一幅图片的暗通道图；正式基于这样的一个想法和统计的规律（可以把这个统计规律当作一条定理），何等人提出了去雾的算法，该算法在大量的户外有雾图片的应用中得以验证其准确性；并且在去雾的过程中，他们也同时得到了原图的景深图片，因为雾的厚度一定程度上代表了景深。 为了验证上述提到的统计规律，作者对5000张无雾图的暗通道的强度进行了统计，可以发现暗通道图中大部分像素都是0，而且全部像素都集中于0-50之间，可以说暗通道图是稀疏的，这一点对于我们下边的公式推导至关重要。 去雾过程首先，在计算机视觉和计算机图形学领域中有一个应用广泛的有雾图片表达公式，如下：$$ I(x)=J(x)t(x)+A(1-t(x)) \\tag{1}$$ 其中，\\( I(x) \\)代表待处理的图（雾图），\\( J(X) \\)代表真实的图（无雾图）也就是我们想要得到的图片，\\( t(X) \\)代表透射率，A代表了全局大气光值（global atmospheric light）接下来我们的整体思路就是求解式(1)，得到\\( J(X) \\)，但是在式(1)里\\( t(X) \\)和A都是未知的，这可如何是好呀？ 别急，我们首先看看暗通道的数学定义吧，如下式： $$ J^{dark}(x)= \\min_{c\\in{(r,g,b)}} {(\\min_{y\\in{\\Omega(x)}} {J^c(y)})} \\tag{2} $$ 其中，等式左边即为暗通道图，等式右边：C代表R，G，B中的某一通道，x代表图中某一像素点，Ω(X)代表像素点X周围的小区域；这个公式的意思可以这样理解：首先取原图每一个像素点中最小的通道值，这样就可以得到一副灰色的图了，然后对这个灰色的图进行最小值滤波（滤波窗口代表了Ω(X)）就得到了暗通道图。 下面我们来看看究竟是如何从公式(1)推出\\( J(X) \\) 的： 首先对(1)两边进行“取最小”的操作，得： $$ \\min_{y\\in{\\Omega(x)}}{(I^c(y))}=\\hat{t}(x) \\min_{y\\in{\\Omega(x)}}{(J^c(y))}+(1-\\hat{t}(x))A^c \\tag{3}$$ 然后等式两边同时除以大气光值A，得： $$ \\min_{y\\in{\\Omega(x)}}{(\\frac{I^c(y)}{A^c})}=\\hat{t}(x) \\min_{y\\in{\\Omega(x)}}{(\\frac{J^c(y)}{A^c})}+(1-\\hat{t}(x)) \\tag{4}$$ 等式两边再次进行“取最小”操作，得： $$ \\min_c \\min_{y\\in{\\Omega(x)}}{(\\frac{I^c(y)}{A^c})}=\\hat{t}(x) \\min_c \\min_{y\\in{\\Omega(x)}}{(\\frac{J^c(y)}{A^c})}+(1-\\hat{t}(x)) \\tag{5} $$ 在第一部分我们知道：暗通道图是稀疏的，绝大多数元素为零或者趋向于零，好，有了这个认识，我们可以近似地得到如下式子： $$J^{dark}(x)=\\min_c \\min_{y\\in{\\Omega(x)}} J^c (y)=0 \\tag{6}$$ 因此，式（5）中的\\( t(x) \\)的系数项近似为0，于是整理可得： $$\\hat{t}(x)=1-\\min_c \\min_{y\\in{\\Omega(x)}}\\frac{I^c(y)}{A^c} \\tag{7}$$ 有了式(7)我们只要求得大气光值A 就可以得到\\( t(X) \\) 了，而得到\\( t(X) \\)之后我们根据式(1)即可求出\\( J(X) \\)看似无解的问题就这样解决啦！多亏了暗通道的先验知识。 作者做了进一步的限制，如下： $$\\hat{t}(x)=1-w\\min_c \\min_{y\\in{\\Omega(x)}}\\frac{I^c(y)}{A^c} $$ 与式（7）相比作者增加了参数w，作者认为为了保持图像的真实感需要保留少量的“雾气”，因此w接近于1，论文中w=0.95； 最终J的表达式为： $$J(x)=\\frac{I(x)-A}{max(t(x),t_0)}+A$$ 作者又引入了\\( t_0 \\)限制：t(X)不能太小了，因为过小之后图片会出现亮的区域特别亮，暗淡的区域特变暗的情况；论文中的t0=0.1； 到此这篇论文的精髓就介绍完啦，下一篇从OpenCV c++代码实现的角度进行一些介绍。","tags":[{"name":"暗通道去雾","slug":"暗通道去雾","permalink":"https://coderskychen.cn/tags/暗通道去雾/"},{"name":"C\\C++","slug":"C-C","permalink":"https://coderskychen.cn/tags/C-C/"},{"name":"opencv","slug":"opencv","permalink":"https://coderskychen.cn/tags/opencv/"}]},{"title":"视频转码：OSMF框架下的.f4f转化为.flv","date":"2015-12-05T01:32:06.000Z","path":"2015/12/05/视频转码：OSMF框架下的-f4f转化为-flv/","text":"用C编写的OSMF框架下的.f4f转化为.flv尊重原创，转载请注明出处。 OSMF框架即为开源多媒体框架（open source media frame），据老师介绍，它的一个好处就是可以在视频文件的头部预留一个区域，这个区域是使用者可以“私人定制”的，所以增强了对视频的控制，更加灵活； 下面介绍.f4f和.flv的格式： flv的格式如图： f4f的格式如图： 在f4f格式中，有固定的四个字节表示”mdat”，而mdat的前边四个字节代表的是视频内容长度，比如 20 09 B4 B5，代表的十进制长度就是‭537506997‬，单位：bytes见下图，会更加直观一些： 转换的思路：根据把f4f文件的”mdat”作为标识符；首先入读f4f字节序列，搜索第一个6D646174（即为”mdat”，因为可能有多个mdat），找到之后计算出视频内容的长度，进而可以确定视频内容序列在f4f文件中的位置；最后新建个一个flv文件，把flv固定的“头”与f4f文件中的视频内容拼在一起就ok啦。 需要注意的： 1.读文件时别读到string里边，因为表示形式会改变，比如：00–&gt;30 2.时间主要消耗在字串搜索上边，一般情况下暴力搜索就可以，时间也可以忍受，但是如果处理高并发的情况（同时处理好多文件），就需要优化子串搜索了，代码里写了两个搜索函数，一个BF暴力的，一个sunday算法的 源码在这里","tags":[{"name":"C\\C++","slug":"C-C","permalink":"https://coderskychen.cn/tags/C-C/"},{"name":"视频编解码","slug":"视频编解码","permalink":"https://coderskychen.cn/tags/视频编解码/"}]}]